{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import flax\n",
    "except ImportError:\n",
    "    %pip install -q flax\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    %pip install -q optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "(transforms)=\n",
    "\n",
    "# Kernel Transforms\n",
    "\n",
    "`tinygp` is designed to make it easy to implement new kernels (see {ref}`kernels` for an example), but a particular set of customizations that `tinygp` supports with a high-level interface are coordinate transforms.\n",
    "The basic idea here is that you may want to pass your input coordinates through a linear or non-linear transformation before evaluating one of the standard kernels in that transformed space.\n",
    "This is particularly useful for multivariate inputs where, for example, you may want to capture the different units, or prior covariances between dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "(transforms-dkl)=\n",
    "\n",
    "## Example: Deep kernel lerning\n",
    "\n",
    "The [Deep Kernel Learning](https://arxiv.org/abs/1511.02222) model is an example of a more complicated kernel transform, and since `tinygp` integrates well with libraries like `flax` (see {ref}`modeling`) the implementation of such a model is fairly straightforward.\n",
    "To demonstrate, let's start by sampling a simulated dataset from a step function, a model that a GP would typically struggle to model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random = np.random.default_rng(567)\n",
    "\n",
    "noise = 0.1\n",
    "\n",
    "x = np.sort(random.uniform(-1, 1, 100))\n",
    "y = 2 * (x > 0) - 1 + random.normal(0.0, noise, len(x))\n",
    "t = np.linspace(-1.5, 1.5, 500)\n",
    "\n",
    "plt.plot(t, 2 * (t > 0) - 1, \"k\", lw=1, label=\"truth\")\n",
    "plt.plot(x, y, \".k\", label=\"data\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Then we will fit this model using a model similar to the one described in {ref}`modeling-flax`, except our kernel will include a custom {class}`tinygp.kernels.Transform` that will pass the input coordinates through a (small) neural network before passing them into a {class}`tinygp.kernels.Matern32` kernel.\n",
    "Otherwise, the model and optimization procedure are similar to the ones used in {ref}`modeling-flax`.\n",
    "\n",
    "We compare the performance of the Deep Matern-3/2 kernel (a {class}`tinygp.kernels.Matern32` kernel, with custom neural network transform) to the performance of the same kernel without the transform. The untransformed model doesn't have the capacity to capture our simulated step function, but our transformed model does. In our transformed model, the hyperparameters of our kernel now include the weights of our neural network transform, and we learn those simultaneously with the length scale and amplitude of the `Matern32` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.linen.initializers import zeros\n",
    "\n",
    "from tinygp import GaussianProcess, kernels, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Matern32Loss(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, t):\n",
    "        # Set up a typical Matern-3/2 kernel\n",
    "        log_sigma = self.param(\"log_sigma\", zeros, ())\n",
    "        log_rho = self.param(\"log_rho\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "        base_kernel = jnp.exp(2 * log_sigma) * kernels.Matern32(jnp.exp(log_rho))\n",
    "\n",
    "        # Evaluate and return the GP negative log likelihood as usual\n",
    "        gp = GaussianProcess(\n",
    "            base_kernel, x[:, None], diag=noise**2 + jnp.exp(2 * log_jitter)\n",
    "        )\n",
    "        log_prob, gp_cond = gp.condition(y, t[:, None])\n",
    "        return -log_prob, (gp_cond.loc, gp_cond.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"A small neural network used to non-linearly transform the input data\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=15)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepLoss(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, t):\n",
    "        # Set up a typical Matern-3/2 kernel\n",
    "        log_sigma = self.param(\"log_sigma\", zeros, ())\n",
    "        log_rho = self.param(\"log_rho\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "        base_kernel = jnp.exp(2 * log_sigma) * kernels.Matern32(jnp.exp(log_rho))\n",
    "\n",
    "        # Define a custom transform to pass the input coordinates through our\n",
    "        # `Transformer` network from above\n",
    "        # Note: with recent version of flax, you can't directly vmap modules,\n",
    "        # but we can get around that by explicitly constructing the init and\n",
    "        # apply functions. Ref:\n",
    "        # https://flax.readthedocs.io/en/latest/advanced_topics/lift.html\n",
    "        transform = Transformer()\n",
    "        transform_params = self.param(\"transform\", transform.init, x[:1])\n",
    "        apply_fn = lambda x: transform.apply(transform_params, x)\n",
    "        kernel = transforms.Transform(apply_fn, base_kernel)\n",
    "\n",
    "        # Evaluate and return the GP negative log likelihood as usual with the\n",
    "        # transformed features\n",
    "        gp = GaussianProcess(\n",
    "            kernel, x[:, None], diag=noise**2 + jnp.exp(2 * log_jitter)\n",
    "        )\n",
    "        log_prob, gp_cond = gp.condition(y, t[:, None])\n",
    "\n",
    "        # We return the loss, the conditional mean and variance, and the\n",
    "        # transformed input parameters\n",
    "        return (\n",
    "            -log_prob,\n",
    "            (gp_cond.loc, gp_cond.variance),\n",
    "            (transform(x[:, None]), transform(t[:, None])),\n",
    "        )\n",
    "\n",
    "\n",
    "# Define and train the model\n",
    "def loss_func(model):\n",
    "    def loss(params):\n",
    "        return model.apply(params, x, y, t)[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "models_list, params_list = [], []\n",
    "loss_vals = {}\n",
    "# Plot the results and compare to the true model\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(9, 3))\n",
    "for it, (model_name, model) in enumerate(\n",
    "    zip(\n",
    "        [\"Deep\", \"Matern32\"],\n",
    "        [DeepLoss(), Matern32Loss()],\n",
    "    )\n",
    "):\n",
    "    loss_vals[it] = []\n",
    "    params = model.init(jax.random.PRNGKey(1234), x, y, t)\n",
    "    tx = optax.sgd(learning_rate=1e-4)\n",
    "    opt_state = tx.init(params)\n",
    "\n",
    "    loss = loss_func(model)\n",
    "    loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "    for i in range(1000):\n",
    "        loss_val, grads = loss_grad_fn(params)\n",
    "        updates, opt_state = tx.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        loss_vals[it].append(loss_val)\n",
    "\n",
    "    mu, var = model.apply(params, x, y, t)[1]\n",
    "    ax[it].plot(t, 2 * (t > 0) - 1, \"k\", lw=1, label=\"truth\")\n",
    "    ax[it].plot(x, y, \".k\", label=\"data\")\n",
    "    ax[it].plot(t, mu)\n",
    "    ax[it].fill_between(\n",
    "        t, mu + np.sqrt(var), mu - np.sqrt(var), alpha=0.5, label=\"model\"\n",
    "    )\n",
    "    ax[it].set_xlim(-1.5, 1.5)\n",
    "    ax[it].set_ylim(-1.3, 1.3)\n",
    "    ax[it].set_xlabel(\"x\")\n",
    "    ax[it].set_ylabel(\"y\")\n",
    "    ax[it].set_title(model_name)\n",
    "    _ = ax[it].legend()\n",
    "\n",
    "    models_list.append(model)\n",
    "    params_list.append(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The untransformed `Matern32` model suffers from over-smoothing at the discontinuity, and poor extrapolation performance.\n",
    "The `Deep` model extrapolates well and captures the discontinuity reliably.\n",
    "\n",
    "We can compare the training loss (negative log likelihood) traces for these two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.plot()\n",
    "plt.plot(loss_vals[0], label=\"Deep\")\n",
    "plt.plot(loss_vals[1], label=\"Matern32\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "To inspect what the transformed model is doing under the hood, we can plot the functional form of the transformation, as well as the transformed values of our input coordinates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform, t_transform = models_list[0].apply(params_list[0], x, y, t)[2]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(t, t_transform, \"k\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3)\n",
    "plt.xlabel(\"input data; x\")\n",
    "plt.ylabel(\"transformed data; x'\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(9, 3))\n",
    "for it, (fig_title, feature_input, x_label) in enumerate(\n",
    "    zip([\"Input Data\", \"Transformed Data\"], [x, x_transform], [\"x\", \"x'\"])\n",
    "):\n",
    "    ax[it].plot(feature_input, y, \".k\")\n",
    "    ax[it].set_xlim(-1.5, 1.5)\n",
    "    ax[it].set_ylim(-1.3, 1.3)\n",
    "    ax[it].set_title(fig_title)\n",
    "    ax[it].set_xlabel(x_label)\n",
    "    ax[it].set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The neural network transforms the input feature into a step function like data (as shown in the figures above) before feeding to the base kernel, making it better suited than the baseline model for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
