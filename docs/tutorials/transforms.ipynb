{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6796277-39d4-492b-b589-7800f2447219",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import flax\n",
    "except ImportError:\n",
    "    %pip install -q flax\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    %pip install -q optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4736d-09ad-4613-836a-be9477f2a12d",
   "metadata": {},
   "source": [
    "(transforms)=\n",
    "\n",
    "# Kernel Transforms\n",
    "\n",
    "`tinygp` is designed to make it easy to implement new kernels (see {ref}`kernels` for an example), but a particular set of customizations that `tinygp` supports with a high-level interface are coordinate transforms.\n",
    "The basic idea here is that you may want to pass your input coordinates through a linear or non-linear transformation before evaluating one of the standard kernels in that transformed space.\n",
    "This is particularly useful for multivariate inputs where, for example, you may want to capture the different units, or prior covariances between dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08c4c1-a1b3-4a1e-a2fc-4eb95cd37d33",
   "metadata": {},
   "source": [
    "(transforms-dkl)=\n",
    "\n",
    "## Example: Deep kernel lerning\n",
    "\n",
    "The [Deep Kernel Learning](https://arxiv.org/abs/1511.02222) model is an example of a more complicated kernel transform, and since `tinygp` integrates well with libraries like `flax` (see {ref}`modeling`) the implementation of such a model is fairly straightforward.\n",
    "To demonstrate, let's start by sampling a simulated dataset from a step function, a model that a GP would typically struggle to model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd3917-a738-47d6-841f-e0bfb9e68fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random = np.random.default_rng(567)\n",
    "\n",
    "noise = 0.1\n",
    "\n",
    "x = np.sort(random.uniform(-1, 1, 100))\n",
    "y = 2 * (x > 0) - 1 + random.normal(0.0, noise, len(x))\n",
    "t = np.linspace(-1.5, 1.5, 500)\n",
    "\n",
    "plt.plot(t, 2 * (t > 0) - 1, \"k\", lw=1, label=\"truth\")\n",
    "plt.plot(x, y, \".k\", label=\"data\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c104d3d-ff22-4372-8503-2c0f4931c85c",
   "metadata": {},
   "source": [
    "Then we will fit this model using a model similar to the one described in {ref}`modeling-flax`, except our kernel will include a custom {class}`tinygp.kernels.Transform` that will pass the input coordinates through a (small) neural network before passing them into a {class}`tinygp.kernels.Matern32` kernel.\n",
    "Otherwise, the model and optimization procedure are similar to the ones used in {ref}`modeling-flax`.\n",
    "\n",
    "In the example below, we compare the performance of Deep Matern32 Kernel (with custom neural network transform) with a straight Matern32 Kernel (without transform) at the discontinuity (sudden jump/fall) region in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0065dea-379a-4e0d-8cf2-f460c8126a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen.initializers import zeros\n",
    "from tinygp import kernels, transforms, GaussianProcess\n",
    "\n",
    "\n",
    "class Matern32Loss(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, t):\n",
    "        # Set up a typical Matern-3/2 kernel\n",
    "        log_sigma = self.param(\"log_sigma\", zeros, ())\n",
    "        log_rho = self.param(\"log_rho\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "        base_kernel = jnp.exp(2 * log_sigma) * kernels.Matern32(\n",
    "            jnp.exp(log_rho)\n",
    "        )\n",
    "\n",
    "        # Evaluate and return the GP negative log likelihood as usual\n",
    "        gp = GaussianProcess(\n",
    "            base_kernel, x[:, None], diag=noise**2 + jnp.exp(2 * log_jitter)\n",
    "        )\n",
    "        log_prob, gp_cond = gp.condition(y, t[:, None])\n",
    "        return -log_prob, (gp_cond.loc, gp_cond.variance)\n",
    "\n",
    "\n",
    "# Define a small neural network used to non-linearly transform the input data in our model\n",
    "class Transformer(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=15)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepLoss(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, t):\n",
    "        # Set up a typical Matern-3/2 kernel\n",
    "        log_sigma = self.param(\"log_sigma\", zeros, ())\n",
    "        log_rho = self.param(\"log_rho\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "        base_kernel = jnp.exp(2 * log_sigma) * kernels.Matern32(\n",
    "            jnp.exp(log_rho)\n",
    "        )\n",
    "\n",
    "        # Define a custom transform to pass the input coordinates through our `Transformer`\n",
    "        # network from above\n",
    "        transform = Transformer()\n",
    "        kernel = transforms.Transform(transform, base_kernel)\n",
    "\n",
    "        # Evaluate and return the GP negative log likelihood as usual with the transformed features\n",
    "        gp = GaussianProcess(\n",
    "            kernel, x[:, None], diag=noise**2 + jnp.exp(2 * log_jitter)\n",
    "        )\n",
    "        log_prob, gp_cond = gp.condition(y, t[:, None])\n",
    "        return (\n",
    "            -log_prob,\n",
    "            (gp_cond.loc, gp_cond.variance),\n",
    "            transform(x[:, None]),\n",
    "            transform(t[:, None]),\n",
    "        )\n",
    "\n",
    "\n",
    "# Define and train the model\n",
    "def loss_func(model):\n",
    "    def loss(params):\n",
    "        return model.apply(params, x, y, t)[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "models_list, params_list = [], []\n",
    "# Plot the results and compare to the true model\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(9, 3))\n",
    "for it, (model_name, model) in enumerate(\n",
    "    zip(\n",
    "        [\"Deep Kernel\", \"Matern-3/2\"],\n",
    "        [DeepLoss(), Matern32Loss()],\n",
    "    )\n",
    "):\n",
    "    params = model.init(jax.random.PRNGKey(1234), x, y, t)\n",
    "    tx = optax.sgd(learning_rate=1e-4)\n",
    "    opt_state = tx.init(params)\n",
    "\n",
    "    loss = loss_func(model)\n",
    "    loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "    for i in range(1000):\n",
    "        loss_val, grads = loss_grad_fn(params)\n",
    "        updates, opt_state = tx.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "    mu, var = model.apply(params, x, y, t)[1]\n",
    "    ax[it].plot(t, 2 * (t > 0) - 1, \"k\", lw=1, label=\"truth\")\n",
    "    ax[it].plot(x, y, \".k\", label=\"data\")\n",
    "    ax[it].plot(t, mu)\n",
    "    ax[it].fill_between(\n",
    "        t, mu + np.sqrt(var), mu - np.sqrt(var), alpha=0.5, label=\"model\"\n",
    "    )\n",
    "    ax[it].set_xlim(-1.5, 1.5)\n",
    "    ax[it].set_ylim(-1.3, 1.3)\n",
    "    ax[it].set_xlabel(\"x\")\n",
    "    ax[it].set_ylabel(\"y\")\n",
    "    ax[it].set_title(model_name)\n",
    "    _ = ax[it].legend()\n",
    "\n",
    "    models_list.append(model)\n",
    "    params_list.append(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d5f08",
   "metadata": {},
   "source": [
    "The straight Matern32 kernel suffers from over-smoothing at the discontinuity region, and there are large error bars near the train points. But the Deep Kernel fits better on this dataset with smaller errors and handles the sudden jump in the data adequately.\n",
    "\n",
    "The neural network transforms the input data features into a step function like data (as shown in the figure below) before feeding to the base kernel, making it better suited than the straight base Kernel for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b035-513a-4215-87fd-1a83b52ebd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform, t_transform = models_list[0].apply(params_list[0], x, y, t)[2:]\n",
    "\n",
    "\n",
    "plt.plot(t, t_transform, \"k\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.3, 1.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"x'\")\n",
    "plt.title(\"Transformation of input feature\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(9, 4))\n",
    "\n",
    "\n",
    "for it, (fig_title, feature_input, x_label) in enumerate(\n",
    "    zip([\"Original Data\", \"Transformed Data\"], [x, x_transform], [\"x\", \"x'\"])\n",
    "):\n",
    "    ax[it].plot(feature_input, y, \".k\")\n",
    "    ax[it].set_xlim(-1.5, 1.5)\n",
    "    ax[it].set_ylim(-1.3, 1.3)\n",
    "    ax[it].set_title(fig_title)\n",
    "    ax[it].set_xlabel(x_label)\n",
    "    ax[it].set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d4465",
   "metadata": {},
   "source": [
    "Now we fit a straight Matern32 Kernel on the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd64ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Matern32Loss()\n",
    "params = model.init(jax.random.PRNGKey(1234), x_transform, y, t_transform)\n",
    "tx = optax.sgd(learning_rate=1e-4)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "loss = loss_func(model)\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "for i in range(1000):\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "\n",
    "mu, var = model.apply(params, x_transform, y, t_transform)[1]\n",
    "fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(9, 4))\n",
    "for it, (input_data, input_line, x_label, fig_title) in enumerate(\n",
    "    zip(\n",
    "        [x_transform, x],\n",
    "        [t_transform, t],\n",
    "        [\"x'\", \"x\"],\n",
    "        [\n",
    "            \"Trained Matern-3/2 kernel on transformed data\",\n",
    "            \"Trained Matern-3/2 kernel predictions on original data\",\n",
    "        ],\n",
    "    )\n",
    "):\n",
    "    ax[it].plot(input_line, 2 * (t > 0) - 1, \"k\", lw=1, label=\"truth\")\n",
    "    ax[it].plot(input_data, y, \".k\", label=\"data\")\n",
    "    ax[it].plot(input_line, mu)\n",
    "    ax[it].fill_between(\n",
    "        input_line.squeeze(),\n",
    "        mu + np.sqrt(var),\n",
    "        mu - np.sqrt(var),\n",
    "        alpha=0.5,\n",
    "        label=\"model\",\n",
    "    )\n",
    "    ax[it].set_xlim(-1.5, 1.5)\n",
    "    ax[it].set_ylim(-1.3, 1.3)\n",
    "    ax[it].set_xlabel(x_label)\n",
    "    ax[it].set_ylabel(\"y\")\n",
    "    ax[it].set_title(fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d805ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
