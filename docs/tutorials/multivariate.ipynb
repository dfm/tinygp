{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e39d4c-9caf-4435-a8ed-aba9544bdfac",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import jaxopt\n",
    "except ImportError:\n",
    "    %pip install -q jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8253c-8c31-49be-b7d0-ead5b1dccfb5",
   "metadata": {},
   "source": [
    "(multivariate)=\n",
    "\n",
    "# Multivariate Data\n",
    "\n",
    "````{admonition} Warning\n",
    ":class: warning\n",
    "\n",
    "If you previously used [`george`](https://george.readthedocs.io/en/latest/user/kernels/#stationary-kernels), the way `tinygp` handles multivariate inputs is subtly different.\n",
    "For kernels that depend on the squared distance between points (e.g. {class}`tinygp.kernels.ExpSquared`), the behavior is the same, but for kernels that depend on the absolute distance (e.g. {class}`tinygp.kernels.Matern32`), the argument to the kernel is computed as:\n",
    "\n",
    "```python\n",
    "r = jnp.sum(jnp.abs((x1 - x2) / scale)))\n",
    "```\n",
    "\n",
    "instead of\n",
    "\n",
    "```python\n",
    "r = jnp.sqrt(jnp.sum(jnp.square((x1 - x2) / scale))))\n",
    "```\n",
    "\n",
    "as it was when using `george`.\n",
    "This is indicated in the {ref}`api-kernels` section of the API docs, where the argument of each kernel is defined.\n",
    "\n",
    "It is possible to change this behavior by specifying your preferred {class}`tinygp.kernels.stationary.Distance` metric using the `distance` argument to any {class}`tinygp.kernels.Stationary` kernel.\n",
    "\n",
    "Also, `tinygp` does not require that you specify dimension of the kernel using an `ndim` parameter when instantiating the kernel.\n",
    "The parameters of the kernel must, however, be broadcastable to the dimension of your inputs.\n",
    "\n",
    "````\n",
    "\n",
    "In this tutorial we will discuss how to handle multi-dimensional _input_ data using `tinygp`.\n",
    "All of the built-in kernels, support vector inputs out of the box, and this tutorial goes through some possible modeling choices in this context.\n",
    "`tinygp` also supports structured [pytree](https://jax.readthedocs.io/en/latest/pytrees.html) inputs when you use custom kernels as discussed in {ref}`derivative`, or more complex transformations as discussed in {ref}`transforms`.\n",
    "\n",
    "In the case of vector inputs, most kernels have a \"scale\" parameter that scales the input coordinates before evaluating the kernel.\n",
    "This parameter can have any shape that is broadcastable to your input dimension.\n",
    "For example, the following shows a few different equivalent formulations of the same kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d43bc9-c22b-4863-ad79-aa5f9caf81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from tinygp import kernels\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "ndim = 3\n",
    "X = np.random.default_rng(1).normal(size=(10, ndim))\n",
    "\n",
    "# This kernel is equivalent...\n",
    "scale = 1.5\n",
    "kernel1 = kernels.Matern32(scale)\n",
    "\n",
    "# ... to manually scaling the input coordinates\n",
    "kernel0 = kernels.Matern32()\n",
    "np.testing.assert_allclose(kernel0(X / scale, X / scale), kernel1(X, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddea8f-413e-4c31-9610-79e90eaf41aa",
   "metadata": {},
   "source": [
    "As discussed below, you can construct more sophisticated scalings, including covariances, by introducing multivariate transforms.\n",
    "\n",
    "As discussed in {ref}`transforms`, these transforms work by passing the input variables through some function before evaluating the kernel model on the _transformed_ variables.\n",
    "The transforms provided by `tinygp`—including {class}`tinygp.transforms.Cholesky`, {class}`tinygp.transforms.Linear`, and {class}`tinygp.transforms.Subspace`—are all designed to operate on vector inputs and offer linear transformations of the inputs.\n",
    "You can use custom transforms to build even more expressive models (see {ref}`transforms`).\n",
    "In this tutorial, we will use the {class}`tinygp.transforms.Cholesky` transform to learn covariances between input dimensions, while the {class}`tinygp.transforms.Subspace` transform could be used to restrict a kernel model to be applied to a subset of the input dimensions.\n",
    "\n",
    "## Simulated data\n",
    "\n",
    "To demonstrate how to use `tinygp` to model multivariate data, let's start by simulating a dataset with 2-dimensional inputs and non-uniform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fc2f1-2359-4f69-bdac-28baad1d86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random = np.random.default_rng(48392)\n",
    "X = random.uniform(-5, 5, (100, 2))\n",
    "yerr = 0.1\n",
    "y = np.sin(X[:, 0]) * np.cos(X[:, 1] + X[:, 0]) + yerr * random.normal(size=len(X))\n",
    "\n",
    "# For plotting predictions on a grid\n",
    "x_grid, y_grid = np.linspace(-5, 5, 100), np.linspace(-5, 5, 50)\n",
    "x_, y_ = np.meshgrid(x_grid, y_grid)\n",
    "y_true = np.sin(x_) * np.cos(x_ + y_)\n",
    "X_pred = np.vstack((x_.flatten(), y_.flatten())).T\n",
    "\n",
    "# For plotting covariance ellipses\n",
    "theta = np.linspace(0, 2 * np.pi, 500)[None, :]\n",
    "ellipse = 0.5 * np.concatenate((np.cos(theta), np.sin(theta)), axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pcolor(x_grid, y_grid, y_true, vmin=y_true.min(), vmax=y_true.max())\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, ec=\"black\", vmin=y_true.min(), vmax=y_true.max())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b24325-d261-4f40-8486-7644a7ce2c10",
   "metadata": {},
   "source": [
    "In this figure, the value of the noise-free underlying model is plotted as an image, and the data points are over-plotted on the same color scale.\n",
    "\n",
    "## A model with anisotropic scales\n",
    "\n",
    "Now, let's fit this simulated dataset using a simple multivariate kernel that has a parameter describing the length scale in each dimension independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae53528-5d8f-4ae5-bf44-69abf7773a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "from tinygp import GaussianProcess, kernels, transforms\n",
    "\n",
    "\n",
    "def train_gp(nparams, build_gp_func):\n",
    "    @jax.jit\n",
    "    def loss(params):\n",
    "        return -build_gp_func(params).log_probability(y)\n",
    "\n",
    "    params = {\n",
    "        \"log_amp\": np.float64(0.0),\n",
    "        \"log_scale\": np.zeros(nparams),\n",
    "    }\n",
    "    solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "    soln = solver.run(params)\n",
    "    return build_gp_func(soln.params)\n",
    "\n",
    "\n",
    "def build_gp_uncorr(params):\n",
    "    kernel = jnp.exp(params[\"log_amp\"]) * transforms.Linear(\n",
    "        jnp.exp(-params[\"log_scale\"]), kernels.ExpSquared()\n",
    "    )\n",
    "    return GaussianProcess(kernel, X, diag=yerr**2)\n",
    "\n",
    "\n",
    "uncorr_gp = train_gp(2, build_gp_uncorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35092c9c-87ce-4573-b444-ff07f7d9b9a1",
   "metadata": {},
   "source": [
    "Based on this fit, we can plot the model predictions and compare to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564506e-7de3-431b-81c3-868e5c04c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = uncorr_gp.condition(y, X_pred).gp.loc.reshape(y_true.shape)\n",
    "xy = ellipse / uncorr_gp.kernel.kernel2.scale[:, None]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "axes[0].plot(xy[0], xy[1], \"--k\", lw=0.5)\n",
    "axes[0].pcolor(x_, y_, y_pred, vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, ec=\"black\", vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[1].pcolor(x_, y_, y_pred - y_true, vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"uncorrelated kernel\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "_ = axes[1].set_title(\"residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f9dfb-c582-4cc2-adff-cc85b90c95d7",
   "metadata": {},
   "source": [
    "In the left-hand panel shows the model prediction on the same scale as the ground truth plot above.\n",
    "The dotted ellipse in the middle of this panel shows the maximum likelihood scale in the input space.\n",
    "This is axis aligned since our model only includes per-dimension length scales, with no prior covariance. \n",
    "The right-hand panel shows the difference between the model prediction and the truth, again on the same scale.\n",
    "\n",
    "## A model with correlated inputs\n",
    "\n",
    "The model in the previous section didn't do a terrible job, but it seems likely that we could make better predictions by taking into account the covariances between inputs.\n",
    "To do this with `tinygp`, we can use one a kernel transform, in this case the {class}`tinygp.transforms.Cholesky` transform.\n",
    "The Cholesky transform works by transforming the input coordinates $x$ to\n",
    "\n",
    "$$\n",
    "x^\\prime = L^{-1}\\,x\n",
    "$$\n",
    "\n",
    "where $L$ is a lower triangular matrix.\n",
    "A good parameterization for $L$ is to fit for its `ndim` diagonal elements with a constraint that they remain positive, and its `(ndim-1) * ndim` off diagonal elements, which need not be positive.\n",
    "The `Cholesky` transform includes a {func}`tinygp.transforms.Cholesky.from_parameters` constructor (which we use here) to help when using this parameterization.\n",
    "\n",
    "Using this parameterization, we can fit this model and plot the results as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017c3f0-9951-48b1-97e3-019aa5096a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gp_corr(params):\n",
    "    kernel = jnp.exp(params[\"log_amp\"]) * transforms.Cholesky.from_parameters(\n",
    "        jnp.exp(params[\"log_scale\"][:2]),\n",
    "        params[\"log_scale\"][2:],\n",
    "        kernels.ExpSquared(),\n",
    "    )\n",
    "    return GaussianProcess(kernel, X, diag=yerr**2)\n",
    "\n",
    "\n",
    "corr_gp = train_gp(3, build_gp_corr)\n",
    "\n",
    "y_pred = corr_gp.condition(y, X_pred).gp.loc.reshape(y_true.shape)\n",
    "xy = corr_gp.kernel.kernel2.factor @ ellipse\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "axes[0].plot(xy[0], xy[1], \"--k\", lw=0.5)\n",
    "axes[0].pcolor(x_, y_, y_pred, vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, ec=\"black\", vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[1].pcolor(x_, y_, y_pred - y_true, vmin=y_true.min(), vmax=y_true.max())\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"correlated kernel\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "_ = axes[1].set_title(\"residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96806e-7436-45c5-93a7-e6f864c0e267",
   "metadata": {},
   "source": [
    "In this case, the input correlations are aligned with the shape of the true function, and our predictions have significantly smaller error, especially near the edges of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bab1c-2a77-4f0d-8e87-1ff7a01bce06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
