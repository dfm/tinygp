{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03b69b-0047-424e-b4c6-3bcb8f551786",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    %pip install -q optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5f580-005f-43d5-9056-28f1885873eb",
   "metadata": {},
   "source": [
    "(kernels)=\n",
    "\n",
    "# Custom Kernels\n",
    "\n",
    "One of the goals of the `tinygp` interface design was to make the kernel building framework flexible and easily extensible.\n",
    "In this tutorial, we demontrate this interface with two examples; one simple, and one more complicated.\n",
    "Besides describing this interface, we also show how `tinygp` can support arbitrary [JAX pytrees](https://jax.readthedocs.io/en/latest/pytrees.html) as input.\n",
    "\n",
    "## Example: Spectral mixture kernel\n",
    "\n",
    "In this section, we will implement the \"spectral mixture kernel\" proposed by [Gordon Wilson & Adams (2013)](https://arxiv.org/abs/1302.4245).\n",
    "It would be possible to implement this using sums of built in kernels, but the interface seems better if we implement a custom kernel and I expect that we'd get somewhat better performance for mixtures with many components.\n",
    "\n",
    "Now, let's implement this kernel in a way that `tinygp` understands.\n",
    "When doing this, you will subclass {class}`tinygp.kernels.Kernel` and implement the {func}`tinygp.kernels.Kernel.evaluate` method.\n",
    "One very important thing to note here is that `evaluate` will always be called via `vmap`, so you should write your `evaluate` method to operate on a **single pair of inputs** and let `vmap` handle the broadcasting sematics for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5434aa-21fc-4a44-b626-8e1f52fad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class SpectralMixture(tinygp.kernels.Kernel):\n",
    "    def __init__(self, weight, scale, freq):\n",
    "        self.weight = jnp.atleast_1d(weight)\n",
    "        self.scale = jnp.atleast_1d(scale)\n",
    "        self.freq = jnp.atleast_1d(freq)\n",
    "\n",
    "    def evaluate(self, X1, X2):\n",
    "        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n",
    "        return jnp.sum(\n",
    "            self.weight\n",
    "            * jnp.prod(\n",
    "                jnp.exp(-2 * jnp.pi**2 * tau**2 / self.scale**2)\n",
    "                * jnp.cos(2 * jnp.pi * self.freq * tau),\n",
    "                axis=0,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf88a8d-d88f-4878-b731-d990be77f65c",
   "metadata": {},
   "source": [
    "Now let's implement the  simulate some data from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba81a1a-5e3a-42d4-b0bb-bdab9cc14f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def build_gp(theta):\n",
    "    kernel = SpectralMixture(\n",
    "        jnp.exp(theta[\"log_weight\"]),\n",
    "        jnp.exp(theta[\"log_scale\"]),\n",
    "        jnp.exp(theta[\"log_freq\"]),\n",
    "    )\n",
    "    return tinygp.GaussianProcess(\n",
    "        kernel, t, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n",
    "    )\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"log_weight\": np.log([1.0, 1.0]),\n",
    "    \"log_scale\": np.log([10.0, 20.0]),\n",
    "    \"log_freq\": np.log([1.0, 1.0 / 3.0]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "    \"mean\": 0.0,\n",
    "}\n",
    "\n",
    "random = np.random.default_rng(546)\n",
    "t = np.sort(random.uniform(0, 10, 50))\n",
    "true_gp = build_gp(params)\n",
    "y = true_gp.sample(jax.random.PRNGKey(123))\n",
    "\n",
    "plt.plot(t, y, \".k\")\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.title(\"simulated data\")\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b4279-f681-49c2-8735-23ed5b8890c8",
   "metadata": {},
   "source": [
    "One thing to note here is that we've used named parameters in a dictionary, instead of an array of parameters as in some of the other examples.\n",
    "This would be awkward (but not impossible) to fit using `scipy`, so instead we'll use [`optax`](https://github.com/deepmind/optax) for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec74d15-3917-4203-8fea-8285b00bd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.value_and_grad\n",
    "def loss(theta):\n",
    "    return -build_gp(theta).log_probability(y)\n",
    "\n",
    "\n",
    "opt = optax.sgd(learning_rate=3e-4)\n",
    "opt_state = opt.init(params)\n",
    "for i in range(1000):\n",
    "    loss_val, grads = loss(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "opt_gp = build_gp(params)\n",
    "tau = np.linspace(0, 5, 500)\n",
    "plt.plot(tau, true_gp.kernel(tau[:1], tau)[0], \"--k\", label=\"true kernel\")\n",
    "plt.plot(tau, opt_gp.kernel(tau[:1], tau)[0], label=\"inferred kernel\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$\\tau$\")\n",
    "plt.ylabel(r\"$k(\\tau)$\")\n",
    "_ = plt.xlim(tau.min(), tau.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b0c45-1bc8-49c5-98f0-63aa09ad9cdf",
   "metadata": {},
   "source": [
    "Using our optimized model, over-plot the conditional predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e81fe-771e-4d45-ab63-4cdb66811e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 12, 500)\n",
    "plt.plot(t, y, \".k\", label=\"data\")\n",
    "gp_cond = opt_gp.condition(y, x).gp\n",
    "mu, var = gp_cond.loc, gp_cond.variance\n",
    "plt.fill_between(\n",
    "    x,\n",
    "    mu + np.sqrt(var),\n",
    "    mu - np.sqrt(var),\n",
    "    color=\"C0\",\n",
    "    alpha=0.5,\n",
    "    label=\"conditional\",\n",
    ")\n",
    "plt.plot(x, mu, color=\"C0\", lw=2)\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a6246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
