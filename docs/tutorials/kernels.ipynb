{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03b69b-0047-424e-b4c6-3bcb8f551786",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    %pip install -q optax\n",
    "\n",
    "try:\n",
    "    import jaxopt\n",
    "except ImportError:\n",
    "    %pip install -q jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5f580-005f-43d5-9056-28f1885873eb",
   "metadata": {},
   "source": [
    "(kernels)=\n",
    "\n",
    "# Tutorial: Custom Kernels & Pytree Data\n",
    "\n",
    "One of the goals of the `tinygp` interface design was to make the kernel building framework flexible and easily extensible.\n",
    "In this tutorial, we demontrate this interface with two examples; one simple, and one more complicated.\n",
    "Besides describing this interface, we also show how `tinygp` can support arbitrary [JAX pytrees](https://jax.readthedocs.io/en/latest/pytrees.html) as input.\n",
    "\n",
    "## Example: Spectral mixture kernel\n",
    "\n",
    "In this section, we will implement the \"spectral mixture kernel\" proposed by [Gordon Wilson & Adams (2013)](https://arxiv.org/abs/1302.4245).\n",
    "It would be possible to implement this using sums of built in kernels, but the interface seems better if we implement a custom kernel and I expect that we'd get somewhat better performance for mixtures with many components.\n",
    "\n",
    "Now, let's implement this kernel in a way that `tinygp` understands.\n",
    "When doing this, you will subclass {class}`tinygp.kernels.Kernel` and implement the {func}`tinygp.kernels.Kernel.evaluate` method.\n",
    "One very important thing to note here is that `evaluate` will always be called via `vmap`, so you should write your `evaluate` method to operate on a **single pair of inputs** and let `vmap` handle the broadcasting sematics for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5434aa-21fc-4a44-b626-8e1f52fad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class SpectralMixture(tinygp.kernels.Kernel):\n",
    "    def __init__(self, weight, scale, freq):\n",
    "        self.weight = jnp.atleast_1d(weight)\n",
    "        self.scale = jnp.atleast_1d(scale)\n",
    "        self.freq = jnp.atleast_1d(freq)\n",
    "\n",
    "    def evaluate(self, X1, X2):\n",
    "        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n",
    "        return jnp.sum(\n",
    "            self.weight\n",
    "            * jnp.prod(\n",
    "                jnp.exp(-2 * jnp.pi**2 * tau**2 / self.scale**2)\n",
    "                * jnp.cos(2 * jnp.pi * self.freq * tau),\n",
    "                axis=-1,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf88a8d-d88f-4878-b731-d990be77f65c",
   "metadata": {},
   "source": [
    "Now let's implement the  simulate some data from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba81a1a-5e3a-42d4-b0bb-bdab9cc14f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def build_gp(theta):\n",
    "    kernel = SpectralMixture(\n",
    "        jnp.exp(theta[\"log_weight\"]),\n",
    "        jnp.exp(theta[\"log_scale\"]),\n",
    "        jnp.exp(theta[\"log_freq\"]),\n",
    "    )\n",
    "    return tinygp.GaussianProcess(\n",
    "        kernel, t, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n",
    "    )\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"log_weight\": np.log([1.0, 1.0]),\n",
    "    \"log_scale\": np.log([10.0, 20.0]),\n",
    "    \"log_freq\": np.log([1.0, 1.0 / 3.0]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "    \"mean\": 0.0,\n",
    "}\n",
    "\n",
    "random = np.random.default_rng(546)\n",
    "t = np.sort(random.uniform(0, 10, 50))\n",
    "true_gp = build_gp(params)\n",
    "y = true_gp.sample(jax.random.PRNGKey(123))\n",
    "\n",
    "plt.plot(t, y, \".k\")\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.title(\"simulated data\")\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b4279-f681-49c2-8735-23ed5b8890c8",
   "metadata": {},
   "source": [
    "One thing to note here is that we've used named parameters in a dictionary, instead of an array of parameters as in some of the other examples.\n",
    "This would be awkward (but not impossible) to fit using `scipy`, so instead we'll use [`optax`](https://github.com/deepmind/optax) for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec74d15-3917-4203-8fea-8285b00bd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.value_and_grad\n",
    "def loss(theta):\n",
    "    return -build_gp(theta).condition(y)\n",
    "\n",
    "\n",
    "opt = optax.sgd(learning_rate=3e-4)\n",
    "opt_state = opt.init(params)\n",
    "for i in range(1000):\n",
    "    loss_val, grads = loss(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "opt_gp = build_gp(params)\n",
    "tau = np.linspace(0, 5, 500)\n",
    "plt.plot(tau, true_gp.kernel(tau[:1], tau)[0], \"--k\", label=\"true kernel\")\n",
    "plt.plot(tau, opt_gp.kernel(tau[:1], tau)[0], label=\"inferred kernel\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$\\tau$\")\n",
    "plt.ylabel(r\"$k(\\tau)$\")\n",
    "_ = plt.xlim(tau.min(), tau.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b0c45-1bc8-49c5-98f0-63aa09ad9cdf",
   "metadata": {},
   "source": [
    "Using our optimized model, over-plot the conditional predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e81fe-771e-4d45-ab63-4cdb66811e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 12, 500)\n",
    "plt.plot(t, y, \".k\", label=\"data\")\n",
    "mu, var = opt_gp.predict(y, x, return_var=True)\n",
    "plt.fill_between(\n",
    "    x,\n",
    "    mu + np.sqrt(var),\n",
    "    mu - np.sqrt(var),\n",
    "    color=\"C0\",\n",
    "    alpha=0.5,\n",
    "    label=\"conditional\",\n",
    ")\n",
    "plt.plot(x, mu, color=\"C0\", lw=2)\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631382c2-94ec-404a-9731-7ec9d745d2ce",
   "metadata": {},
   "source": [
    "(kernels-derivative)=\n",
    "\n",
    "## Example: Latent GPs & their derivatives\n",
    "\n",
    "We will next build a custom kernel that is commonly used when studying radial velocity observations of exoplanet-hosting stars, as described by [Rajpaul et al. (2015)](https://arxiv.org/abs/1506.07304).\n",
    "Take a look at that paper for more details about the math, but the tl;dr is that we want to model a set parallel, but qualitatively different, time series, using a latent Gaussian process.\n",
    "The interesting part of this model is that Rajpaul et al. model the observations as arbitrary linear combinations of the process and its first time derivative, and they work through the derivation of the resulting kernel function.\n",
    "\n",
    "In this tutorial, we will implement this kernel using `tinygp`—something that is significantly more annoying to do with other Gaussian process frameworks (trust me, I've tried!)—and demonstrate a few key features along the way.\n",
    "\n",
    "### The kernel\n",
    "\n",
    "The kernel matrix described by [Rajpaul et al. (2015)](https://arxiv.org/abs/1506.07304) is a block matrix where each element is a linear combination of the latent kernel and its first and second derivatives, where the relevant coefficients depend on the \"class\" of each pair of observations.\n",
    "This means that our input data needs to include, at each observation, the time `t` (our input coordinate in this case) and an integer class label `label`.\n",
    "As discussed below, we will structure our data in such a way that we can treat each input as being a tuple `(t, label)`.\n",
    "In this case, we will unpack our inputs `t, label = X` and treat `t` and `label` as scalars.\n",
    "Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa8e6e-11b5-4c02-88c7-0fd798240f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerivativeKernel(tinygp.kernels.Kernel):\n",
    "    \"\"\"A custom kernel based on Rajpaul et al. (2015)\n",
    "\n",
    "    Args:\n",
    "        kernel: The kernel function describing the latent process. This can be any other\n",
    "            ``tinygp`` kernel.\n",
    "        coeff_prim: The primal coefficients for each class. This can be thought of as how\n",
    "            much the latent process itself projects into the observations for that class.\n",
    "            This should be an array with an entry for each class of observation.\n",
    "        coeff_deriv: The derivative coefficients for each class. This should have the same\n",
    "            shape as ``coeff_prim``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, coeff_prim, coeff_deriv):\n",
    "        self.kernel = kernel\n",
    "        self.coeff_prim, self.coeff_deriv = jnp.broadcast_arrays(\n",
    "            jnp.asarray(coeff_prim), jnp.asarray(coeff_deriv)\n",
    "        )\n",
    "\n",
    "    def evaluate(self, X1, X2):\n",
    "        t1, label1 = X1\n",
    "        t2, label2 = X2\n",
    "\n",
    "        # Differentiate the kernel function: the first derivative wrt x1\n",
    "        Kp = jax.grad(self.kernel.evaluate, argnums=0)\n",
    "\n",
    "        # ... and the second derivative\n",
    "        Kpp = jax.grad(Kp, argnums=1)\n",
    "\n",
    "        # Evaluate the kernel matrix and all of its relevant derivatives\n",
    "        K = self.kernel.evaluate(t1, t2)\n",
    "        d2K_dx1dx2 = Kpp(t1, t2)\n",
    "\n",
    "        # For stationary kernels, these are related just by a minus sign, but we'll\n",
    "        # evaluate them both separately for generality's sake\n",
    "        dK_dx2 = jax.grad(self.kernel.evaluate, argnums=1)(t1, t2)\n",
    "        dK_dx1 = Kp(t1, t2)\n",
    "\n",
    "        # Extract the coefficients\n",
    "        a1 = self.coeff_prim[label1]\n",
    "        a2 = self.coeff_prim[label2]\n",
    "        b1 = self.coeff_deriv[label1]\n",
    "        b2 = self.coeff_deriv[label2]\n",
    "\n",
    "        # Construct the matrix element\n",
    "        return (\n",
    "            a1 * a2 * K\n",
    "            + a1 * b2 * dK_dx2\n",
    "            + b1 * a2 * dK_dx1\n",
    "            + b1 * b2 * d2K_dx1dx2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d33f9f-753a-46be-bff8-9afc9b73f8d9",
   "metadata": {},
   "source": [
    "Now that we have this definition, we can plot what the kernel functions look like for different latent processes.\n",
    "Don't worry too much about the syntax here, but we're plotting two classes of observations where the first class is just a direct observation of the latent process and the second observes the time derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e132b8-3ab6-4ca8-b72c-6ffd0c798655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_kernel(latent_kernel):\n",
    "    kernel = DerivativeKernel(latent_kernel, [1.0, 0.0], [0.0, 1.0])\n",
    "\n",
    "    N = 500\n",
    "    dt = np.linspace(-7.5, 7.5, N)\n",
    "\n",
    "    k00 = kernel(\n",
    "        (jnp.zeros((1)), jnp.zeros((1), dtype=int)),\n",
    "        (dt, np.zeros(N, dtype=int)),\n",
    "    )[0]\n",
    "    k11 = kernel(\n",
    "        (jnp.zeros((1)), jnp.ones((1), dtype=int)), (dt, np.ones(N, dtype=int))\n",
    "    )[0]\n",
    "    k01 = kernel(\n",
    "        (jnp.zeros((1)), jnp.zeros((1), dtype=int)),\n",
    "        (dt, np.ones(N, dtype=int)),\n",
    "    )[0]\n",
    "    k10 = kernel(\n",
    "        (jnp.zeros((1)), jnp.ones((1), dtype=int)),\n",
    "        (dt, np.zeros(N, dtype=int)),\n",
    "    )[0]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(dt, k00, label=\"$k[\\Delta t^{(0,0)}]$\", lw=1)\n",
    "    plt.plot(dt, k01, label=\"$k[\\Delta t^{(0,1)}]$\", lw=1)\n",
    "    plt.plot(dt, k10, label=\"$k[\\Delta t^{(1,0)}]$\", lw=1)\n",
    "    plt.plot(dt, k11, label=\"$k[\\Delta t^{(1,1)}]$\", lw=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(r\"$\\Delta t$\")\n",
    "    plt.xlim(dt.min(), dt.max())\n",
    "\n",
    "\n",
    "plot_kernel(tinygp.kernels.Matern52(scale=1.5))\n",
    "plt.title(\"Matern-5/2\")\n",
    "\n",
    "plot_kernel(\n",
    "    tinygp.kernels.ExpSquared(scale=2.5)\n",
    "    * tinygp.kernels.ExpSineSquared(period=2.5, gamma=0.5)\n",
    ")\n",
    "_ = plt.title(\"Quasiperiodic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095b8ad-6e79-48fe-bf9c-43748eb9beb6",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Given this custom kernel definition, let's simulate some data and fit for the kernel parameters.\n",
    "In this case, we'll use the product of an {class}`kernels.ExpSquared` and a {class}`kernels.ExpSineSquared` kernel for our latent process.\n",
    "Note that we're not including an amplitude parameter in our latent process, since that will be captured by the coefficients in our `DerivativeKernel` defined above.\n",
    "Using this latent process, we simulate an unbalanced dataset with two classes and plot the simulated data below, with class offsets for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ec42a-b4b5-43e7-83a8-bdde71b2214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_kernel = tinygp.kernels.ExpSquared(\n",
    "    scale=1.5\n",
    ") * tinygp.kernels.ExpSineSquared(period=2.5, gamma=0.5)\n",
    "kernel = DerivativeKernel(latent_kernel, [1.0, 0.5], [-0.1, 0.3])\n",
    "\n",
    "random = np.random.default_rng(5678)\n",
    "t1 = np.sort(random.uniform(0, 10, 200))\n",
    "label1 = np.zeros_like(t1, dtype=int)\n",
    "t2 = np.sort(random.uniform(0, 10, 300))\n",
    "label2 = np.ones_like(t2, dtype=int)\n",
    "X = (np.append(t1, t2), np.append(label1, label2))\n",
    "\n",
    "gp = tinygp.GaussianProcess(kernel, X, diag=1e-5)\n",
    "y = gp.sample(jax.random.PRNGKey(1234))\n",
    "\n",
    "subset = np.append(\n",
    "    random.integers(len(t1), size=50),\n",
    "    len(t1) + random.integers(len(t2), size=15),\n",
    ")\n",
    "X_obs = (X[0][subset], X[1][subset])\n",
    "y_obs = y[subset] + 0.1 * random.normal(size=len(subset))\n",
    "\n",
    "offset = 2.5\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t1, y[: len(t1)] + 0.5 * offset, label=\"class 0\")\n",
    "plt.plot(t2, y[len(t1) :] - 0.5 * offset, label=\"class 1\")\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f467a-d37f-4c6f-866f-d967b2c96d80",
   "metadata": {},
   "source": [
    "Then, we fit the simulated data, by optimizing for the maximum likelihood kernel parameters using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e20dae-a20b-4d80-beda-bb5f3516d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    latent_kernel = tinygp.kernels.ExpSquared(\n",
    "        scale=jnp.exp(params[\"log_scale\"])\n",
    "    ) * tinygp.kernels.ExpSineSquared(\n",
    "        period=jnp.exp(params[\"log_period\"]), gamma=params[\"gamma\"]\n",
    "    )\n",
    "    kernel = DerivativeKernel(\n",
    "        latent_kernel, params[\"coeff_prim\"], params[\"coeff_deriv\"]\n",
    "    )\n",
    "    return tinygp.GaussianProcess(\n",
    "        kernel, X_obs, diag=jnp.exp(params[\"log_diag\"])\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.condition(y_obs)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_scale\": np.log(1.5),\n",
    "    \"log_period\": np.log(2.5),\n",
    "    \"gamma\": np.float64(0.5),\n",
    "    \"coeff_prim\": np.array([1.0, 0.5]),\n",
    "    \"coeff_deriv\": np.array([-0.1, 0.3]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60e2b9-b9b3-4d70-b622-78693507c057",
   "metadata": {},
   "source": [
    "And plot the resulting inference.\n",
    "Of particular note here, even for the sparsely sampled \"class 1\" dataset, we get robust predictions for the expected process, since we have more finely sampled observations in \"class 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4c132-4348-4505-ae1b-b1e4b827eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params)\n",
    "mu, var = gp.predict(y_obs, X, return_var=True)\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t1, y[: len(t1)] + 0.5 * offset, \"k\", label=\"truth\")\n",
    "plt.plot(t2, y[len(t1) :] - 0.5 * offset, \"k\")\n",
    "\n",
    "for c in [0, 1]:\n",
    "    delta = offset * (0.5 - c)\n",
    "    m = X[1] == c\n",
    "    # plt.plot(X[0][m], delta + mu[m], color=f\"C{c}\")\n",
    "    plt.fill_between(\n",
    "        X[0][m],\n",
    "        delta + mu[m] + 2 * np.sqrt(var[m]),\n",
    "        delta + mu[m] - 2 * np.sqrt(var[m]),\n",
    "        color=f\"C{c}\",\n",
    "        alpha=0.5,\n",
    "        label=f\"inferred class {c}\",\n",
    "    )\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc227-e72b-4366-a9cb-3f5a969510c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
