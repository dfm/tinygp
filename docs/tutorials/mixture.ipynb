{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mixture)=\n",
    "\n",
    "# Tutorial: Mixture of Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to model a dataset using a mixture of GPs.\n",
    "For example, the data might have both systematic effects and a physical signal that can be modeled using a GP.\n",
    "I konw of a few examples where this method has been used in the context of time series analysis for the discovery of transiting exoplanets (for example, [Aigrain et al. 2016](https://arxiv.org/abs/1603.09167) and [Luger et al. 2016](https://arxiv.org/abs/1607.00524)), but I'm sure that these aren't the earliest references.\n",
    "The idea is pretty simple: if your model is a mixture of two GPs (with covariance matrices $K_1$ and $K_2$ respectively), this is equivalent to a single GP where the kernel is the sum of two kernels, one for each component ($K = K_1 + K_2$).\n",
    "In this case, the equation for the predictive mean conditioned on a dataset $\\boldsymbol{y}$ is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = (K_1 + K_2)\\,(K_1 + K_2 + N)^{-1} \\, \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "where $N$ is the (possibly diagonal) matrix describing the measurement uncertainties.\n",
    "It turns out that the equation for computing the predictive mean for component 1 is simply\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_1 = K_1\\,(K_1 + K_2 + N)^{-1} \\, \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "and the equivalent expression can be written for component 2.\n",
    "\n",
    "This can be implemented in `tinygp` using the new `kernel` keyword argument in the `predict` method.\n",
    "To demonstrate this, let's start by generating a synthetic dataset.\n",
    "Component 1 is a systematic signal that depends on two input parameters ($t$ and $\\theta$ following Aigrain) and component 2 is a quasiperiodic oscillation that is the target of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tinygp import GaussianProcess, kernels, transforms\n",
    "\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "random = np.random.default_rng(123)\n",
    "N = 256\n",
    "t = np.sort(random.uniform(0, 10, N))\n",
    "theta = random.uniform(-np.pi, np.pi, N)\n",
    "X = np.vstack((t, theta)).T\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    params = jnp.exp(params)\n",
    "    kernel1 = params[0] * kernels.Matern32(params[1:3])\n",
    "    kernel2 = params[3] * transforms.Subspace(\n",
    "        0,\n",
    "        kernels.ExpSquared(params[4])\n",
    "        * kernels.ExpSineSquared(period=params[5], gamma=params[6]),\n",
    "    )\n",
    "    kernel = kernel1 + kernel2\n",
    "    return GaussianProcess(kernel, X, diag=params[7])\n",
    "\n",
    "\n",
    "true_params = np.log([2.0, 2.0, 0.8, 2.0, 3.5, 2.0, 10.0, 0.5])\n",
    "gp = build_gp(true_params)\n",
    "y = gp.sample(jax.random.PRNGKey(5678))\n",
    "\n",
    "plt.plot(t, y, \".k\")\n",
    "plt.ylim(-6.5, 6.5)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical (oscillatory) component is not obvious in this dataset because it is swamped by the systematics.\n",
    "Now, we'll find the maximum likelihood hyperparameters by numerically minimizing the negative log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.value_and_grad\n",
    "def loss(params):\n",
    "    return -build_gp(params).condition(y)\n",
    "\n",
    "\n",
    "soln = minimize(loss, true_params, jac=True)\n",
    "print(soln.success, soln.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the trick from above to compute the prediction of component 1 and remove it to see the periodic signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predictive means - note the \"kernel\" argument\n",
    "gp = build_gp(soln.x)\n",
    "mu1 = gp.predict(y, X, kernel=gp.kernel.kernel1)\n",
    "mu2 = gp.predict(y, X, kernel=gp.kernel.kernel2)\n",
    "\n",
    "plt.plot(t, y, \".k\", mec=\"none\", alpha=0.3)\n",
    "plt.plot(t, y - mu1, \".k\")\n",
    "plt.plot(t, mu2)\n",
    "\n",
    "plt.ylim(-6.5, 6.5)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the original dataset is plotted in light gray points and the \"de-trended\" data with component 1 removed is plotted as black points.\n",
    "The prediction of the GP model for component 2 is shown as a blue line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
