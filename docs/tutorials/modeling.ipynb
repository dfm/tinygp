{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import numpyro\n",
    "except ImportError:\n",
    "    %pip uninstall -y jax jaxlib\n",
    "    %pip install -q numpyro jax jaxlib\n",
    "\n",
    "try:\n",
    "    import arviz\n",
    "except ImportError:\n",
    "    %pip install arviz\n",
    "\n",
    "try:\n",
    "    import flax\n",
    "except ImportError:\n",
    "    %pip install -q flax\n",
    "\n",
    "try:\n",
    "    import optax\n",
    "except ImportError:\n",
    "    %pip install -q optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "(modeling)=\n",
    "\n",
    "# Modeling Frameworks\n",
    "\n",
    "One of the key design decisions made in the `tinygp` API is that it shouldn't place strong constraints on your modeling on inference choices.\n",
    "Most existing Python-based GP libraries require a large amount of buy in from users, and we wanted to avoid that here.\n",
    "That being said, you will be required to buy into `jax` as your computational backend, but there exists a rich ecosystem of modeling frameworks that should all be compatible with `tinygp`.\n",
    "In this tutorial, we demonstrate how you might use `tinygp` combined with some popular `jax`-based modeling frameworks:\n",
    "\n",
    "1. {ref}`modeling-flax`, and\n",
    "2. {ref}`modeling-numpyro`.\n",
    "\n",
    "Similar examples should be possible with other libraries like [TensorFlow Probability](https://www.tensorflow.org/probability), [PyMC (version > 4.0)](https://docs.pymc.io), [mcx](https://github.com/rlouf/mcx), or [BlackJAX](https://github.com/blackjax-devs/blackjax), to name a few.\n",
    "\n",
    "To begin with, let's simulate a dataset that we can use for our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random = np.random.default_rng(42)\n",
    "\n",
    "t = np.sort(\n",
    "    np.append(\n",
    "        random.uniform(0, 3.8, 28),\n",
    "        random.uniform(5.5, 10, 18),\n",
    "    )\n",
    ")\n",
    "yerr = random.uniform(0.08, 0.22, len(t))\n",
    "y = (\n",
    "    0.2 * (t - 5)\n",
    "    + np.sin(3 * t + 0.1 * (t - 5) ** 2)\n",
    "    + yerr * random.normal(size=len(t))\n",
    ")\n",
    "\n",
    "true_t = np.linspace(0, 10, 100)\n",
    "true_y = 0.2 * (true_t - 5) + np.sin(3 * true_t + 0.1 * (true_t - 5) ** 2)\n",
    "\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3)\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "_ = plt.title(\"simulated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "(modeling-flax)=\n",
    "\n",
    "## Optimization with flax & optax\n",
    "\n",
    "Using our simulated dataset from above, we may want to find the maximum (marginal) likelihood hyperparameters for a GP model.\n",
    "One popular modeling framework that we can use for this task is [`flax`](https://github.com/google/flax).\n",
    "A benefit of integrating with `flax` is that we can then easily combine our GP model with other machine learning models for all sorts of fun results (see {ref}`transforms-dkl`, for example).\n",
    "\n",
    "To set up our model, we define a custom `linen.Module`, and optimize it's parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.linen.initializers import zeros\n",
    "\n",
    "from tinygp import GaussianProcess, kernels\n",
    "\n",
    "\n",
    "class GPModule(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, yerr, y, t):\n",
    "        mean = self.param(\"mean\", zeros, ())\n",
    "        log_jitter = self.param(\"log_jitter\", zeros, ())\n",
    "\n",
    "        log_sigma1 = self.param(\"log_sigma1\", zeros, ())\n",
    "        log_rho1 = self.param(\"log_rho1\", zeros, ())\n",
    "        log_tau = self.param(\"log_tau\", zeros, ())\n",
    "        kernel1 = (\n",
    "            jnp.exp(2 * log_sigma1)\n",
    "            * kernels.ExpSquared(jnp.exp(log_tau))\n",
    "            * kernels.Cosine(jnp.exp(log_rho1))\n",
    "        )\n",
    "\n",
    "        log_sigma2 = self.param(\"log_sigma2\", zeros, ())\n",
    "        log_rho2 = self.param(\"log_rho2\", zeros, ())\n",
    "        kernel2 = jnp.exp(2 * log_sigma2) * kernels.Matern32(jnp.exp(log_rho2))\n",
    "\n",
    "        kernel = kernel1 + kernel2\n",
    "        gp = GaussianProcess(kernel, x, diag=yerr**2 + jnp.exp(log_jitter), mean=mean)\n",
    "\n",
    "        log_prob, gp_cond = gp.condition(y, t)\n",
    "        return -log_prob, gp_cond.loc\n",
    "\n",
    "\n",
    "def loss(params):\n",
    "    return model.apply(params, t, yerr, y, true_t)[0]\n",
    "\n",
    "\n",
    "model = GPModule()\n",
    "params = model.init(jax.random.PRNGKey(0), t, yerr, y, true_t)\n",
    "tx = optax.sgd(learning_rate=3e-3)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "losses = []\n",
    "for i in range(1001):\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    losses.append(loss_val)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"negative log likelihood\")\n",
    "_ = plt.xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Our `Module` defined above also returns the conditional predictions, that we can compare to the true model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.apply(params, t, yerr, y, true_t)[1]\n",
    "\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3, label=\"truth\")\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(true_t, pred, label=\"max likelihood model\")\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.legend()\n",
    "_ = plt.title(\"maximum likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "(modeling-numpyro)=\n",
    "\n",
    "## Sampling with numpyro\n",
    "\n",
    "Perhaps we're not satisfied with just a point estimate of our hyperparameters and we want to instead compute posterior expectations.\n",
    "One tool for doing that is [`numpyro`](https://github.com/pyro-ppl/numpyro), which offers Markov chain Monte Carlo (MCMC) and variational inference methods.\n",
    "As a demonstration, here's how we would set up the model from above and run MCMC in `numpyro`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "prior_sigma = 5.0\n",
    "\n",
    "\n",
    "def numpyro_model(t, yerr, y=None):\n",
    "    mean = numpyro.sample(\"mean\", dist.Normal(0.0, prior_sigma))\n",
    "    jitter = numpyro.sample(\"jitter\", dist.HalfNormal(prior_sigma))\n",
    "\n",
    "    sigma1 = numpyro.sample(\"sigma1\", dist.HalfNormal(prior_sigma))\n",
    "    rho1 = numpyro.sample(\"rho1\", dist.HalfNormal(prior_sigma))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfNormal(prior_sigma))\n",
    "    kernel1 = sigma1**2 * kernels.ExpSquared(tau) * kernels.Cosine(rho1)\n",
    "\n",
    "    sigma2 = numpyro.sample(\"sigma2\", dist.HalfNormal(prior_sigma))\n",
    "    rho2 = numpyro.sample(\"rho2\", dist.HalfNormal(prior_sigma))\n",
    "    kernel2 = sigma2**2 * kernels.Matern32(rho2)\n",
    "\n",
    "    kernel = kernel1 + kernel2\n",
    "    gp = GaussianProcess(kernel, t, diag=yerr**2 + jitter, mean=mean)\n",
    "    numpyro.sample(\"gp\", gp.numpyro_dist(), obs=y)\n",
    "\n",
    "    if y is not None:\n",
    "        numpyro.deterministic(\"pred\", gp.condition(y, true_t).gp.loc)\n",
    "\n",
    "\n",
    "nuts_kernel = NUTS(numpyro_model, dense_mass=True, target_accept_prob=0.9)\n",
    "mcmc = MCMC(\n",
    "    nuts_kernel,\n",
    "    num_warmup=1000,\n",
    "    num_samples=1000,\n",
    "    num_chains=2,\n",
    "    progress_bar=False,\n",
    ")\n",
    "rng_key = jax.random.PRNGKey(34923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mcmc.run(rng_key, t, yerr, y=y)\n",
    "samples = mcmc.get_samples()\n",
    "pred = samples[\"pred\"].block_until_ready()  # Blocking to get timing right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "When running iterative methods like MCMC, it's always a good idea to check some convergence diagnostics.\n",
    "For that task, let's use [`ArviZ`](https://arviz-devs.github.io):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "data = az.from_numpyro(mcmc)\n",
    "az.summary(data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "And, finally we can plot our posterior inferences of the conditional process, compared to the true model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.percentile(pred, [5, 50, 95], axis=0)\n",
    "plt.fill_between(true_t, q[0], q[2], color=\"C0\", alpha=0.5, label=\"inference\")\n",
    "plt.plot(true_t, q[1], color=\"C0\", lw=2)\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3, label=\"truth\")\n",
    "\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.legend()\n",
    "_ = plt.title(\"posterior inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
