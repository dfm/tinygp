{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9335ef4-9488-427c-b9fd-6ac74d5da1d0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import numpyro\n",
    "except ImportError:\n",
    "    %pip uninstall -y jax jaxlib\n",
    "    %pip install -q numpyro jax jaxlib\n",
    "\n",
    "try:\n",
    "    import arviz\n",
    "except ImportError:\n",
    "    %pip install arviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ce874-83e2-417b-9238-e46d57bd7fd3",
   "metadata": {},
   "source": [
    "(likelihoods)=\n",
    "\n",
    "# Non-Gaussian Likelihoods\n",
    "\n",
    "In this tutorial, we demonstrate how `tinygp` can be used in combination with non-Gaussian observation models.\n",
    "The basic idea is that your `tinygp` model can be a middle layer in your probabilistic model; it doesn't just have to be at the bottom.\n",
    "One issue with this is that the marginalization over process realizations will no longer be analytic, so you'll need to marginalize numerically using Markov chain Monte Carlo (MCMC) or variational inference (VI).\n",
    "Since `tinygp` doesn't include any built-in inference methods you'll need to use a different package, but luckily there are lots of good tools that exist!\n",
    "In this case, we'll use `numpyro`.\n",
    "\n",
    "As our test case, we'll look at counts data with a Poisson observation model where the underlying log-rate is modeled by a Gaussian process (also known as a Cox process).\n",
    "To begin, let's simulate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0047440-ef0e-490c-abc0-45ed3f396ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random = np.random.default_rng(203618)\n",
    "x = np.linspace(-3, 3, 20)\n",
    "true_log_rate = 2 * np.cos(2 * x)\n",
    "y = random.poisson(np.exp(true_log_rate))\n",
    "plt.plot(x, y, \".k\", label=\"data\")\n",
    "plt.plot(x, np.exp(true_log_rate), \"C1\", label=\"true rate\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c174a8a-a441-485d-9b9b-5c061943b50d",
   "metadata": {},
   "source": [
    "## Markov chain Monte Carlo (MCMC)\n",
    "\n",
    "Then we set up the model in `numpyro` and run the MCMC, following the example in {ref}`modeling-numpyro`.\n",
    "The main difference here is that we're using MCMC to marginalize over GP realizations (that is encoded in the following by the fact that the `log_rate` parameter doesn't have the `obs=...` argument set), instead of analytically marginalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8f667-bacb-4385-9369-e831a65d84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from tinygp import kernels, GaussianProcess\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "def model(x, y=None):\n",
    "    # The parameters of the GP model\n",
    "    mean = numpyro.sample(\"mean\", dist.Normal(0.0, 2.0))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(3.0))\n",
    "    rho = numpyro.sample(\"rho\", dist.HalfNormal(10.0))\n",
    "\n",
    "    # Set up the kernel and GP objects\n",
    "    kernel = sigma**2 * kernels.Matern52(rho)\n",
    "    gp = GaussianProcess(kernel, x, diag=1e-5, mean=mean)\n",
    "\n",
    "    # This parameter has shape (num_data,) and it encodes our beliefs about\n",
    "    # the process rate in each bin\n",
    "    log_rate = numpyro.sample(\"log_rate\", gp.numpyro_dist())\n",
    "\n",
    "    # Finally, our observation model is Poisson\n",
    "    numpyro.sample(\"obs\", dist.Poisson(jnp.exp(log_rate)), obs=y)\n",
    "\n",
    "\n",
    "# Run the MCMC\n",
    "nuts_kernel = numpyro.infer.NUTS(model, target_accept_prob=0.9)\n",
    "mcmc = numpyro.infer.MCMC(\n",
    "    nuts_kernel,\n",
    "    num_warmup=1000,\n",
    "    num_samples=1000,\n",
    "    num_chains=2,\n",
    "    progress_bar=False,\n",
    ")\n",
    "rng_key = jax.random.PRNGKey(55873)\n",
    "mcmc.run(rng_key, x, y=y)\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56542a-ef64-4b82-8084-cb74de397b70",
   "metadata": {},
   "source": [
    "We can summarize the MCMC results by plotting our inferred model (here we're showing the 1- and 2-sigma credible regions), and compare it to the known ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af823999-5aa2-4769-a302-b60e5df7d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.percentile(samples[\"log_rate\"], [5, 25, 50, 75, 95], axis=0)\n",
    "plt.plot(x, np.exp(q[2]), color=\"C0\", label=\"MCMC inferred rate\")\n",
    "plt.fill_between(x, np.exp(q[0]), np.exp(q[-1]), alpha=0.3, lw=0, color=\"C0\")\n",
    "plt.fill_between(x, np.exp(q[1]), np.exp(q[-2]), alpha=0.3, lw=0, color=\"C0\")\n",
    "plt.plot(x, np.exp(true_log_rate), \"--\", color=\"C1\", label=\"true rate\")\n",
    "plt.plot(x, y, \".k\", label=\"data\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf5ea7-6d35-406e-8416-0efd3624f3d8",
   "metadata": {},
   "source": [
    "## Stochastic variational inference (SVI)\n",
    "\n",
    "The above results look good and didn't take long to run, but if we had more data the runtime could become prohibitive, since the number of parameters scales with the sizegreater_than the dataset.\n",
    "In cases like this, we can compute the relevant expectations using stochastic variational inference (SVI) instead of MCMC.\n",
    "Here's one way that you could set up such a model using `tinygp` and `numpyro`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc1154-df51-40bc-958e-9674912f5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y=None):\n",
    "    # The parameters of the GP model\n",
    "    mean = numpyro.param(\"mean\", jnp.zeros(()))\n",
    "    sigma = numpyro.param(\"sigma\", jnp.ones(()), constraint=dist.constraints.positive)\n",
    "    rho = numpyro.param(\"rho\", 2 * jnp.ones(()), constraint=dist.constraints.positive)\n",
    "\n",
    "    # Set up the kernel and GP objects\n",
    "    kernel = sigma**2 * kernels.Matern52(rho)\n",
    "    gp = GaussianProcess(kernel, x, diag=1e-5, mean=mean)\n",
    "\n",
    "    # This parameter has shape (num_data,) and it encodes our beliefs about\n",
    "    # the process rate in each bin\n",
    "    log_rate = numpyro.sample(\"log_rate\", gp.numpyro_dist())\n",
    "\n",
    "    # Finally, our observation model is Poisson\n",
    "    numpyro.sample(\"obs\", dist.Poisson(jnp.exp(log_rate)), obs=y)\n",
    "\n",
    "\n",
    "def guide(x, y=None):\n",
    "    mu = numpyro.param(\n",
    "        \"log_rate_mu\", jnp.zeros_like(x) if y is None else jnp.log(y + 1)\n",
    "    )\n",
    "    sigma = numpyro.param(\n",
    "        \"log_rate_sigma\",\n",
    "        jnp.ones_like(x),\n",
    "        constraint=dist.constraints.positive,\n",
    "    )\n",
    "    numpyro.sample(\"log_rate\", dist.Independent(dist.Normal(mu, sigma), 1))\n",
    "\n",
    "\n",
    "optim = numpyro.optim.Adam(0.01)\n",
    "svi = numpyro.infer.SVI(model, guide, optim, numpyro.infer.Trace_ELBO(10))\n",
    "results = svi.run(jax.random.PRNGKey(55873), 3000, x, y=y, progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37105dde-e708-46d7-8fd1-19387b7d765d",
   "metadata": {},
   "source": [
    "As above, we can plot our inferred conditional model and compare it to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b351b-8fe2-4a0b-9aac-fc8955ecf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = results.params[\"log_rate_mu\"]\n",
    "sigma = results.params[\"log_rate_sigma\"]\n",
    "plt.plot(x, np.exp(mu), color=\"C0\", label=\"VI inferred rate\")\n",
    "plt.fill_between(\n",
    "    x,\n",
    "    np.exp(mu - 2 * sigma),\n",
    "    np.exp(mu + 2 * sigma),\n",
    "    alpha=0.3,\n",
    "    lw=0,\n",
    "    color=\"C0\",\n",
    ")\n",
    "plt.fill_between(x, np.exp(mu - sigma), np.exp(mu + sigma), alpha=0.3, lw=0, color=\"C0\")\n",
    "plt.plot(x, np.exp(true_log_rate), \"--\", color=\"C1\", label=\"true rate\")\n",
    "plt.plot(x, y, \".k\", label=\"data\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff3aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
