{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import jaxopt\n",
    "except ImportError:\n",
    "    %pip install -q jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "(quasisep-custom)=\n",
    "\n",
    "# Custom Quasiseparable Kernels\n",
    "\n",
    "````{admonition} Warning\n",
    ":class: warning\n",
    "\n",
    "This implementation of quasiseparable kernels is still experimental, and the models in this tutorial depend on low-level features that are subject to change.\n",
    "````\n",
    "\n",
    "The quasiseparable kernels built in to `tinygp` are all designed to be used with one-dimensional data (see {ref}`api-kernels-quasisep`), but one of the key selling points of the `tinygp` implementation over other similar projects (e.g. [celerite](https://celerite.readthedocs.io), [celerite2](https://celerite2.readthedocs.io), [S+LEAF](https://obswww.unige.ch/~delisle/spleaf/doc/)), is that it has a model building interface that is more expressive and flexible.\n",
    "In this tutorial, we present some examples of the kinds of extensions that are possible within this framework.\n",
    "This will be one of the most technical `tinygp` tutorials, and the implementation details are likely to change in future versions; you have been warned!\n",
    "\n",
    "\n",
    "(quasisep-custom-multi)=\n",
    "\n",
    "## Multivariate quasiseparable kernels\n",
    "\n",
    "[Gordon et al. (2020)](https://arxiv.org/abs/2007.05799) demonstrated how the `celerite` algorithm could be extended to model \"rectangular\" data (e.g. parallel time series), and here we'll implement a slightly more general model that includes the Gordon et al. (2020) model as a special case.\n",
    "But to start, let's implement something that is very similar to the simplest model from Gordon et al. (2020).\n",
    "In this model, we have a single underlying Gaussian process, and each data point is generated from that process with a different amplitude.\n",
    "To add a little more structure to the model, we'll imagine that we're modeling \"multi-band\" data where each observation is indexed by \"time\" (or some other one-dimensional input coordinate) and it's band ID (an integer).\n",
    "\n",
    "We're not going to go into the mathematical details here (stay tuned for more details, or maybe even a publication?), but the methods that our custom kernel needs to overload here are {func}`tinygp.kernels.quasisep.Quasisep.coord_to_sortable` and {func}`tinygp.kernels.quasisep.Quasisep.observation_model`.\n",
    "The first method (`coord_to_sortable`) takes our structured input (in this case a tuple with time as the first element, and band ID as the second), and returns a scalar that is sorted in the dataset (in this case, the time coordinate is what we need).\n",
    "The second method (`observation_model`) is where the magic happens.\n",
    "To get the behavior that we want here, we overload the `observation_model` by scaling the observation for each data point by the amplitude in that band.\n",
    "\n",
    "Here's how we would implement this in `tinygp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tinygp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "@tinygp.helpers.dataclass\n",
    "class Multiband(tinygp.kernels.quasisep.Wrapper):\n",
    "    amplitudes: jnp.ndarray\n",
    "\n",
    "    def coord_to_sortable(self, X):\n",
    "        return X[0]\n",
    "\n",
    "    def observation_model(self, X):\n",
    "        return self.amplitudes[X[1]] * self.kernel.observation_model(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Some notes here:\n",
    "\n",
    "1. We're using {class}`tinygp.kernels.quasisep.Wrapper` as our base class (instead of {class}`tinygp.kernels.quasisep.Quasisep`), since it provides some help when writing a custom kernel that wraps another quasiseparable kernel.\n",
    "2. We've decorated our class with the `@tinygp.helpers.dataclass` decorator which, while not strictly necessary, can make our lives a little easier.\n",
    "\n",
    "Now that we have this implementation, let's build an example model with a {class}`tinygp.kernels.quasisep.Matern52` as our base kernel and 3 bands.\n",
    "Then we'll sample from it to get a sense for what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_multiband_sample(kernel):\n",
    "    gp = tinygp.GaussianProcess(kernel, X)\n",
    "    y = gp.sample(jax.random.PRNGKey(849))\n",
    "    for i in np.unique(band_id):\n",
    "        plt.axhline(-7 * i, color=\"k\")\n",
    "        plt.plot(t[band_id == i], y[band_id == i] - 7 * i, label=f\"band {i + 1}\")\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(-21, 7)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y + offset\")\n",
    "    plt.legend(fontsize=10, loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random = np.random.default_rng(394)\n",
    "t = np.sort(random.uniform(0, 10, 700))\n",
    "band_id = random.choice([0, 1, 2], size=len(t))\n",
    "X = (t, band_id)\n",
    "\n",
    "kernel = Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=1.5),\n",
    "    amplitudes=jnp.array([3.1, -1.1, 3.7]),\n",
    ")\n",
    "\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This model is very similar to the baseline model introduced by [Gordon et al. (2020; their Equation 13)](https://arxiv.org/abs/2007.05799), with the added generalization that we're not restricted to rectangular data: the observations don't need to be simultaneous.\n",
    "While Gordon et al. (2020) showed that this model could be useful, it's not actually very expressive, so let's take another step.\n",
    "\n",
    "The most obvious generalization of this simple model is to take the sum of several `Multiband` kernels with different amplitudes and, optionally, different underlying processes.\n",
    "As an example, here's what happens if we take the sum of two of our custom kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=1.5),\n",
    "    amplitudes=jnp.array([0.9, 0.7, -1.1]),\n",
    ")\n",
    "kernel += Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=0.5),\n",
    "    amplitudes=jnp.array([1.1, -1.7, 1.5]),\n",
    ")\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This is already a much more expressive kernel, with some shared structure between the two bands, but this relationship is much less restrictive.\n",
    "\n",
    "We can also reproduce the full-rank Kronecker model from [Gordon et al. (2020)](https://arxiv.org/abs/2007.05799) using this same infrastructure.\n",
    "In that case, we need to sum the same number of `Multiband` kernels as there are bands, using the same baseline kernel for each.\n",
    "Then, if we call the $N_\\mathrm{band} \\times N_\\mathrm{band}$ cross-band covariance matrix $R$ following Gordon et al. (2020), and take its Cholesky factorization $R = L\\,L^\\mathrm{T}$, the amplitude for the $n$-th term is the $n$-th row of $L$.\n",
    "For example, we could use an exponential-squared kernel for the cross-band band covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.5**2 * jnp.exp(-0.5 * (jnp.arange(3)[:, None] - jnp.arange(3)[None, :]) ** 2)\n",
    "L = jnp.linalg.cholesky(R)\n",
    "\n",
    "base_kernel = tinygp.kernels.quasisep.Matern52(scale=1.5)\n",
    "kernel = sum(Multiband(kernel=base_kernel, amplitudes=row) for row in L)\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "And now we have a flexible and expressive, but still scalable, kernel for analyzing multi-band data.\n",
    "While we have captured the models studied by [Gordon et al. (2020)](https://arxiv.org/abs/2007.05799) as special cases, the framework proposed here presents several nice generalizations to those results:\n",
    "\n",
    "1. These models are no longer restricted to rectangular data where every band is observed at every time,\n",
    "2. This conceptually different approach where we model the data using a mixture of latent processes, observed using linear projections, permits a wider range of modeling choices, and may be useful for incorporating physical knowledge/interpretation into the model.\n",
    "\n",
    "\n",
    "(quasisep-custom-derivative)=\n",
    "\n",
    "## Quasiseparable kernels & derivative observations\n",
    "\n",
    "Recently, [Delisle et al. (2022)](https://arxiv.org/abs/2201.02440) demonstrated that models of derivative observations (like the ones discussed in the {ref}`derivative` tutorial) could also be implemented using the `celerite` algorithm, without increasing the order of the model.\n",
    "In this example, we demonstrate how such processes can be implemented in `tinygp`.\n",
    "While the resulting model here is qualitatively similar to that introduced by Delisle et al. (2022), the technical implementation details are substantially different, but we'll leave our discussion to the details to a future publication.\n",
    "\n",
    "As our demonstration of this type of modeling, we will roughly reproduce the example described in the {ref}`derivative-latent` section of the {ref}`derivative` tutorial, so you should check that out first if you haven't already.\n",
    "\n",
    "To implement that model in `tinygp`, we will again base our model on the {class}`tinygp.kernels.quasisep.Wrapper` base class, and overload the `coord_to_sortable` and `observation_model` methods.\n",
    "Because of how these quasiseparable kernels are implemented, the derivative model is actually significantly simpler to implement than the general case discussed in {ref}`derivative-latent`, but you'll just have to trust us on this for now until we can reference the technical details elsewhere.\n",
    "\n",
    "Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tinygp.helpers.dataclass\n",
    "class Latent(tinygp.kernels.quasisep.Wrapper):\n",
    "    coeff_prim: jnp.ndarray\n",
    "    coeff_deriv: jnp.ndarray\n",
    "\n",
    "    def coord_to_sortable(self, X):\n",
    "        return X[0]\n",
    "\n",
    "    def observation_model(self, X):\n",
    "        t, label = X\n",
    "        design = self.kernel.design_matrix()\n",
    "        obs = self.kernel.observation_model(t)\n",
    "        obs_prim = jnp.asarray(self.coeff_prim)[label] * obs\n",
    "        obs_deriv = jnp.asarray(self.coeff_deriv)[label] * obs @ design\n",
    "        return obs_prim - obs_deriv\n",
    "\n",
    "\n",
    "base_kernel = tinygp.kernels.quasisep.Matern52(\n",
    "    scale=1.5\n",
    ") * tinygp.kernels.quasisep.Cosine(scale=2.5)\n",
    "kernel = Latent(base_kernel, [0.5, 0.02], [0.01, -0.2])\n",
    "\n",
    "# Unlike the previous derivative observations tutorial, the datapoints here\n",
    "# must be sorted in time.\n",
    "random = np.random.default_rng(5678)\n",
    "t = np.sort(random.uniform(0, 10, 500))\n",
    "label = (random.uniform(0, 1, len(t)) < 0.5).astype(int)\n",
    "X = (t, label)\n",
    "\n",
    "gp = tinygp.GaussianProcess(kernel, X)\n",
    "y = gp.sample(jax.random.PRNGKey(12345))\n",
    "\n",
    "# Select a subset of the data as \"observations\"\n",
    "subset = (1 + 2 * label) * random.uniform(0, 1, len(t)) < 0.3\n",
    "X_obs = (X[0][subset], X[1][subset])\n",
    "y_obs = y[subset] + 0.1 * random.normal(size=subset.sum())\n",
    "\n",
    "offset = 2.5\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "plt.plot(t[label == 0], y[label == 0] + 0.5 * offset, label=\"class 0\")\n",
    "plt.plot(t[label == 1], y[label == 1] - 0.5 * offset, label=\"class 1\")\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.3 * offset, 1.3 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "There are a few things that are worth noting here.\n",
    "First, unlike in the {ref}`derivative` tutorial, our data *must be correctly ordered* (in this case by `t`). This is true for any quasiseparable model, but it isn't going to be checked; you'll just end up with a lot of NaNs.\n",
    "\n",
    "Second, let's take a brief aside into kernel choices here.\n",
    "Not all the kernels defined in the {ref}`api-kernels-quasisep` make sensible choices for problems like this.\n",
    "Some of them (e.g. {class}`tinygp.kernels.quasisep.Celerite`) are not actually differentiable for all values of their parameters.\n",
    "Others may be differentiable, but their derivative processes may not be well-behaved.\n",
    "For example, the time derivative of a Matern-3/2 process ({class}`tinygp.kernels.quasisep.Matern32`), or a simple harmonic oscillator process ({class}`tinygp.kernels.quasisep.SHO`), while defined, will be unphysically noisy as demonstrated below.\n",
    "Therefore, while [Delisle et al. (2022)](https://arxiv.org/abs/2201.02440) advocated for the use of a mixture of SHO kernels, we don't recommend that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_latent_samples(kernel):\n",
    "    yp = tinygp.GaussianProcess(\n",
    "        Latent(kernel, coeff_prim=[1.0, 0.0], coeff_deriv=[0.0, 1.0]), X\n",
    "    ).sample(jax.random.PRNGKey(12345))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "    plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "    plt.plot(t[label == 0], yp[label == 0] + 0.5 * offset, label=\"value\")\n",
    "    plt.plot(t[label == 1], yp[label == 1] - 0.5 * offset, label=\"derivative\")\n",
    "\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(-1.3 * offset, 1.3 * offset)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"y + offset\")\n",
    "    _ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_samples(tinygp.kernels.quasisep.Matern32(1.0, sigma=0.5))\n",
    "plt.title(\"Matern32 kernel\")\n",
    "\n",
    "plot_latent_samples(tinygp.kernels.quasisep.SHO(quality=10.0, omega=1.0))\n",
    "_ = plt.title(\"SHO kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Of the kernels currently implemented in the {ref}`api-kernels-quasisep`, the most sensible ones for our purposes are {class}`tinygp.kernels.quasisep.Matern52`, {class}`tinygp.kernels.quasisep.Cosine`, and sums and products thereof.\n",
    "Samples of these processes and their derivatives are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_samples(tinygp.kernels.quasisep.Matern52(1.0, sigma=0.5))\n",
    "plt.title(\"Matern52 kernel\")\n",
    "\n",
    "plot_latent_samples(tinygp.kernels.quasisep.Cosine(1.0, sigma=0.2))\n",
    "_ = plt.title(\"Cosine kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now that we've been through this aside, let's get back to fitting our simulated data from above.\n",
    "In this case we'll model the data using the same model that we used to simulate it: the product of a `Matern52` kernel and a `Cosine` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    base_kernel = tinygp.kernels.quasisep.Matern52(\n",
    "        scale=jnp.exp(params[\"log_scale\"])\n",
    "    ) * tinygp.kernels.quasisep.Cosine(scale=jnp.exp(params[\"log_period\"]))\n",
    "    kernel = Latent(base_kernel, params[\"coeff_prim\"], params[\"coeff_deriv\"])\n",
    "    return tinygp.GaussianProcess(kernel, X_obs, diag=jnp.exp(params[\"log_diag\"]))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.log_probability(y_obs)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_scale\": np.log(1.5),\n",
    "    \"log_period\": np.log(2.5),\n",
    "    \"coeff_prim\": np.array([0.5, 0.02]),\n",
    "    \"coeff_deriv\": np.array([0.01, -0.2]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Having optimized our model, we can plot the model predictions and compare them to the true model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params)\n",
    "gp_cond = gp.condition(y_obs, X).gp\n",
    "mu, var = gp_cond.loc, gp_cond.variance\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t[label == 0], y[label == 0] + 0.5 * offset, \"k\", label=\"truth\")\n",
    "plt.plot(t[label == 1], y[label == 1] - 0.5 * offset, \"k\")\n",
    "\n",
    "for c in [0, 1]:\n",
    "    delta = offset * (0.5 - c)\n",
    "    m = X[1] == c\n",
    "    plt.fill_between(\n",
    "        X[0][m],\n",
    "        delta + mu[m] + 2 * np.sqrt(var[m]),\n",
    "        delta + mu[m] - 2 * np.sqrt(var[m]),\n",
    "        color=f\"C{c}\",\n",
    "        alpha=0.5,\n",
    "        label=f\"inferred class {c}\",\n",
    "    )\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Like in the {ref}`derivative-latent` tutorial, even though we have many fewer observations of \"class 1\", we are still able to make good predictions for its behavior by propagating information from the observations of \"class 0\".\n",
    "\n",
    "\n",
    "(quasisep-custom-noise)=\n",
    "\n",
    "## Quasiseparable kernels with banded observation models\n",
    "\n",
    "This example doesn't directly fit in this tutorial since we're not implementing a custom kernel, but without a better place to put it we wanted to mention that the {class}`tinygp.noise.Banded` observation noise model is fully compatible with the {class}`tinygp.solvers.quasisep.QuasisepSolver`.\n",
    "The fact that this works stems from the fact that banded matrices can be exactly represented as quasiseparable matrices (see {class}`tinygp.noise.Banded` for more details).\n",
    "\n",
    "This means that it is also possible to implement the class of models supported by the [S+LEAF](https://obswww.unige.ch/~delisle/spleaf/doc/) package using `tinygp`. \n",
    "To demonstrate this, we can roughly reproduce [this example from the S+LEAF documentation](https://obswww.unige.ch/~delisle/spleaf/doc/calib.html).\n",
    "First, we generate a simulated data that is similar to the one from that example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "random = np.random.default_rng(0)\n",
    "\n",
    "nt = 100\n",
    "tmax = 20\n",
    "t = np.sort(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            random.uniform(0, tmax / 3, nt // 2),\n",
    "            random.uniform(2 * tmax / 3, tmax, (nt + 1) // 2),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Quasiperiodic signal\n",
    "amp = 3.0\n",
    "P0 = 5.2\n",
    "dP = 0.75\n",
    "P = P0 + dP * (t / tmax - 1 / 2)\n",
    "y = amp * np.sin(2 * np.pi * t / P)\n",
    "tsmooth = np.linspace(0, tmax, 2000)\n",
    "Psmooth = P0 + dP * (tsmooth / tmax - 1 / 2)\n",
    "ysignal = amp * np.sin(2 * np.pi * tsmooth / Psmooth)\n",
    "dysignal = (\n",
    "    amp\n",
    "    * 2\n",
    "    * np.pi\n",
    "    / Psmooth\n",
    "    * (1 - tsmooth * dP / (tmax * Psmooth))\n",
    "    * np.cos(2 * np.pi * tsmooth / Psmooth)\n",
    ")\n",
    "\n",
    "# Measurement noise\n",
    "yerr_meas = random.uniform(0.5, 1.5, nt)\n",
    "y = y + random.normal(0, yerr_meas)\n",
    "\n",
    "# Calibration noise\n",
    "calib_id = (t // 1).astype(int)\n",
    "caliberr = random.uniform(0.5, 1.5, calib_id[-1] + 1)\n",
    "yerr_calib = caliberr[calib_id]\n",
    "y += random.normal(0, caliberr)[calib_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr = np.sqrt(yerr_meas**2 + yerr_calib**2)\n",
    "plt.plot(tsmooth, ysignal, label=\"truth\")\n",
    "plt.errorbar(t, y, yerr, fmt=\".\", color=\"k\", label=\"data\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Then, we can build the band matrix representation of the observation process.\n",
    "In this case, this is a block diagonal matrix, and here's one way that you might construct it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bandwidth = max(Counter(calib_id).values()) - 1\n",
    "off_diags = np.zeros((nt, bandwidth))\n",
    "for j in range(bandwidth):\n",
    "    m = calib_id[: nt - j - 1] == calib_id[j + 1 :]\n",
    "    off_diags[: nt - j - 1, j][m] = caliberr[calib_id[: nt - j - 1][m]] ** 2\n",
    "\n",
    "noise_model = tinygp.noise.Banded(diag=yerr**2, off_diags=off_diags)\n",
    "\n",
    "plt.imshow(noise_model @ np.eye(nt), cmap=\"gray_r\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "_ = plt.title(\"banded observation model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Then we can use this noise model to fit for the parameters of our model.\n",
    "It would also be possible to fit for the elements of this band matrix by just computing the indices above, instead of the full noise model, but for simplicity we'll just keep the noise model fixed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    kernel = tinygp.kernels.quasisep.SHO(\n",
    "        sigma=jnp.exp(params[\"log_sigma\"]),\n",
    "        quality=jnp.exp(params[\"log_quality\"]),\n",
    "        omega=jnp.exp(params[\"log_omega\"]),\n",
    "    )\n",
    "    return tinygp.GaussianProcess(kernel, t, noise=noise_model)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_sigma\": jnp.log(0.5),\n",
    "    \"log_quality\": jnp.log(5.0),\n",
    "    \"log_omega\": jnp.zeros(()),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")\n",
    "\n",
    "# Plot the results\n",
    "gp = build_gp(soln.params)\n",
    "gp_cond = gp.condition(y, tsmooth).gp\n",
    "mu, var = gp_cond.loc, gp_cond.variance\n",
    "std = np.sqrt(var)\n",
    "\n",
    "plt.plot(tsmooth, ysignal, label=\"truth\")\n",
    "plt.errorbar(t, y, yerr, fmt=\".\", color=\"k\", label=\"data\")\n",
    "plt.plot(tsmooth, mu, color=\"C1\", label=\"predicted\")\n",
    "plt.fill_between(tsmooth, mu - std, mu + std, color=\"C1\", alpha=0.3)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Like in the [S+LEAF docs](https://obswww.unige.ch/~delisle/spleaf/doc/calib.html), if we were to restrict ourselves to a diagonal noise model our interpolated results wouldn't be great, but when we take them into account, the predictions aren't too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tinygp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d20ea8a315da34b3e8fab0dbd7b542a0ef3c8cf12937343660e6bc10a20768e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
