{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-finder",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import jaxopt\n",
    "except ImportError:\n",
    "    %pip install -q jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-inquiry",
   "metadata": {},
   "source": [
    "(quasisep-custom)=\n",
    "\n",
    "# Custom Quasiseparable Kernels\n",
    "\n",
    "````{admonition} Warning\n",
    ":class: warning\n",
    "\n",
    "This implementation of quasiseparable kernels is still experimental, and the models in this tutorial depend on low-level features that are subject to change.\n",
    "````\n",
    "\n",
    "The quasiseparable kernels built in to `tinygp` are all designed to be used with one-dimensional data (see {ref}`api-kernels-quasisep`), but one of the key selling points of the `tinygp` implementation over other similar projects (e.g. [celerite](https://celerite.readthedocs.io), [celerite2](https://celerite2.readthedocs.io), [S+LEAF](https://obswww.unige.ch/~delisle/spleaf/doc/)), is that it has a model building interface that is more expressive and flexible.\n",
    "In this tutorial, we present some examples of the kinds of extensions that are possible within this framework.\n",
    "This will be one of the most technical `tinygp` tutorials, and the implementation details are likely to change in future versions; you have been warned!\n",
    "\n",
    "## Multivariate quasiseparable kernels\n",
    "\n",
    "[Gordon et al. (2020)](https://arxiv.org/abs/2007.05799) demonstrated how the `celerite` algorithm could be extended to model \"rectangular\" data (e.g. parallel time series), and here we'll implement a slightly more general model that includes the Gordon et al. (2020) model as a special case.\n",
    "But to start, let's implement something that is very similar to the simplest model from Gordon et al. (2020).\n",
    "In this model, we have a single underlying Gaussian process, and each data point is generated from that process with a different amplitude.\n",
    "To add a little more structure to the model, we'll imagine that we're modeling \"multi-band\" data where each observation is indexed by \"time\" (or some other one-dimensional input coordinate) and it's band ID (an integer).\n",
    "\n",
    "We're not going to go into the mathematical details here (stay tuned for more details, or maybe even a publication?), but the methods that our custom kernel needs to overload here are {func}`tinygp.kernels.quasisep.Quasisep.coord_to_sortable` and {func}`tinygp.kernels.quasisep.Quasisep.observation_model`.\n",
    "The first method (`coord_to_sortable`) takes our structured input (in this case a tuple with time as the first element, and band ID as the second), and returns a scalar that is sorted in the dataset (in this case, the time coordinate is what we need).\n",
    "The second method (`observation_model`) is where the magic happens.\n",
    "To get the behavior that we want here, we overload the `observation_model` by scaling the observation for each data point by the amplitude in that band.\n",
    "\n",
    "Here's how we would implement this in `tinygp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d537cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tinygp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "@tinygp.helpers.dataclass\n",
    "class Multiband(tinygp.kernels.quasisep.Wrapper):\n",
    "    amplitudes: jnp.ndarray\n",
    "\n",
    "    def coord_to_sortable(self, X):\n",
    "        return X[0]\n",
    "\n",
    "    def observation_model(self, X):\n",
    "        return self.amplitudes[X[1]] * self.kernel.observation_model(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3df4a5",
   "metadata": {},
   "source": [
    "Some notes here:\n",
    "\n",
    "1. We're using {class}`tinygp.kernels.quasisep.Wrapper` as our base class (instead of {class}`tinygp.kernels.quasisep.Quasisep`), since it provides some help when writing a custom kernel that wraps another quasiseparable kernel.\n",
    "2. We've decorated our class with the `@tinygp.helpers.dataclass` decorator which, while not strictly necessary, can make our lives a little easier.\n",
    "\n",
    "Now that we have this implementation, let's build an example model with a {class}`tinygp.kernels.quasisep.Matern52` as our base kernel and 3 bands.\n",
    "Then we'll sample from it to get a sense for what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba51af",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_multiband_sample(kernel):\n",
    "    gp = tinygp.GaussianProcess(kernel, X)\n",
    "    y = gp.sample(jax.random.PRNGKey(849))\n",
    "    for i in np.unique(band_id):\n",
    "        plt.axhline(-7 * i, color=\"k\")\n",
    "        plt.plot(\n",
    "            t[band_id == i], y[band_id == i] - 7 * i, label=f\"band {i + 1}\"\n",
    "        )\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(-21, 7)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y + offset\")\n",
    "    plt.legend(fontsize=10, loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random = np.random.default_rng(394)\n",
    "t = np.sort(random.uniform(0, 10, 700))\n",
    "band_id = random.choice([0, 1, 2], size=len(t))\n",
    "X = (t, band_id)\n",
    "\n",
    "kernel = Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=1.5),\n",
    "    amplitudes=jnp.array([3.1, -1.1, 3.7]),\n",
    ")\n",
    "\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7559a",
   "metadata": {},
   "source": [
    "This model is very similar to the baseline model introduced by [Gordon et al. (2020; their Equation 13)](https://arxiv.org/abs/2007.05799), with the added generalization that we're not restricted to rectangular data: the observations don't need to be simultaneous.\n",
    "While Gordon et al. (2020) showed that this model could be useful, it's not actually very expressive, so let's take another step.\n",
    "\n",
    "The most obvious generalization of this simple model is to take the sum of several `Multiband` kernels with different amplitudes and, optionally, different underlying processes.\n",
    "As an example, here's what happens if we take the sum of two of our custom kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc01726",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=1.5),\n",
    "    amplitudes=jnp.array([0.9, 0.7, -1.1]),\n",
    ")\n",
    "kernel += Multiband(\n",
    "    kernel=tinygp.kernels.quasisep.Matern52(scale=0.5),\n",
    "    amplitudes=jnp.array([1.1, -1.7, 1.5]),\n",
    ")\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae81c4",
   "metadata": {},
   "source": [
    "This is already a much more expressive kernel, with some shared structure between the two bands, but this relationship is much less restrictive.\n",
    "\n",
    "We can also reproduce the full-rank Kronecker model from [Gordon et al. (2020)](https://arxiv.org/abs/2007.05799) using this same infrastructure.\n",
    "In that case, we need to sum the same number of `Multiband` kernels as there are bands, using the same baseline kernel for each.\n",
    "Then, if we call the $N_\\mathrm{band} \\times N_\\mathrm{band}$ cross-band covariance matrix $R$ following Gordon et al. (2020), and take its Cholesky factorization $R = L\\,L^\\mathrm{T}$, the amplitude for the $n$-th term is the $n$-th row of $L$.\n",
    "For example, we could use an exponential-squared kernel for the cross-band band covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.5**2 * jnp.exp(\n",
    "    -0.5 * (jnp.arange(3)[:, None] - jnp.arange(3)[None, :]) ** 2\n",
    ")\n",
    "L = jnp.linalg.cholesky(R)\n",
    "\n",
    "base_kernel = tinygp.kernels.quasisep.Matern52(scale=1.5)\n",
    "kernel = sum(Multiband(kernel=base_kernel, amplitudes=row) for row in L)\n",
    "plot_multiband_sample(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb08b7b",
   "metadata": {},
   "source": [
    "## Quasiseparable kernels & derivative observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81964ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tinygp.helpers.dataclass\n",
    "class Latent(tinygp.kernels.quasisep.Wrapper):\n",
    "    coeff_prim: jnp.ndarray\n",
    "    coeff_deriv: jnp.ndarray\n",
    "\n",
    "    def coord_to_sortable(self, X):\n",
    "        return X[0]\n",
    "\n",
    "    def observation_model(self, X):\n",
    "        t, label = X\n",
    "        design = self.kernel.design_matrix()\n",
    "        obs = self.kernel.observation_model(t)\n",
    "        obs_prim = jnp.asarray(self.coeff_prim)[label] * obs\n",
    "        obs_deriv = jnp.asarray(self.coeff_deriv)[label] * obs @ design\n",
    "        return obs_prim - obs_deriv\n",
    "\n",
    "\n",
    "base_kernel = tinygp.kernels.quasisep.Matern52(\n",
    "    scale=1.5\n",
    ") * tinygp.kernels.quasisep.Cosine(scale=2.5)\n",
    "kernel = Latent(base_kernel, [0.5, 0.02], [0.01, -0.2])\n",
    "\n",
    "# Unlike the previous derivative observations tutorial, the datapoints here\n",
    "# must be sorted in time.\n",
    "random = np.random.default_rng(5678)\n",
    "t = np.sort(random.uniform(0, 10, 500))\n",
    "label = (random.uniform(0, 1, len(t)) < 0.5).astype(int)\n",
    "X = (t, label)\n",
    "\n",
    "gp = tinygp.GaussianProcess(kernel, X)\n",
    "y = gp.sample(jax.random.PRNGKey(12345))\n",
    "\n",
    "# Select a subset of the data as \"observations\"\n",
    "subset = (1 + 2 * label) * random.uniform(0, 1, len(t)) < 0.3\n",
    "X_obs = (X[0][subset], X[1][subset])\n",
    "y_obs = y[subset] + 0.1 * random.normal(size=subset.sum())\n",
    "\n",
    "offset = 2.5\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t[label == 0], y[label == 0] + 0.5 * offset, label=\"class 0\")\n",
    "plt.plot(t[label == 1], y[label == 1] - 0.5 * offset, label=\"class 1\")\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.3 * offset, 1.3 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc33cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    base_kernel = tinygp.kernels.quasisep.Matern52(\n",
    "        scale=jnp.exp(params[\"log_scale\"])\n",
    "    ) * tinygp.kernels.quasisep.Cosine(scale=jnp.exp(params[\"log_period\"]))\n",
    "    kernel = Latent(base_kernel, params[\"coeff_prim\"], params[\"coeff_deriv\"])\n",
    "    return tinygp.GaussianProcess(\n",
    "        kernel, X_obs, diag=jnp.exp(params[\"log_diag\"])\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.log_probability(y_obs)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_scale\": np.log(1.5),\n",
    "    \"log_period\": np.log(2.5),\n",
    "    \"coeff_prim\": np.array([0.5, 0.02]),\n",
    "    \"coeff_deriv\": np.array([0.01, -0.2]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0669f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params)\n",
    "gp_cond = gp.condition(y_obs, X).gp\n",
    "mu, var = gp_cond.loc, gp_cond.variance\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t[label == 0], y[label == 0] + 0.5 * offset, \"k\", label=\"truth\")\n",
    "plt.plot(t[label == 1], y[label == 1] - 0.5 * offset, \"k\")\n",
    "\n",
    "for c in [0, 1]:\n",
    "    delta = offset * (0.5 - c)\n",
    "    m = X[1] == c\n",
    "    plt.fill_between(\n",
    "        X[0][m],\n",
    "        delta + mu[m] + 2 * np.sqrt(var[m]),\n",
    "        delta + mu[m] - 2 * np.sqrt(var[m]),\n",
    "        color=f\"C{c}\",\n",
    "        alpha=0.5,\n",
    "        label=f\"inferred class {c}\",\n",
    "    )\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c524fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
