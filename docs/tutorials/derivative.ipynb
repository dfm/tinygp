{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinygp\n",
    "except ImportError:\n",
    "    %pip install -q tinygp\n",
    "\n",
    "try:\n",
    "    import jaxopt\n",
    "except ImportError:\n",
    "    %pip install -q jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(derivative)=\n",
    "\n",
    "# Derivative Observations & Pytree Data\n",
    "\n",
    "As discussed in [Section 9.4 of R&W](http://www.gaussianprocess.org/gpml/chapters/RW9.pdf), it is possible to build a model that incorporates observations of the _derivative_ of the process.\n",
    "In this case, the kernel becomes:\n",
    "\n",
    "$$\n",
    "\\mathrm{cov}\\left(f_i,\\, f_j\\right) = k(x_i,\\,x_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{cov}\\left(f_i,\\, \\dot{f}_j\\right) = \\frac{\\partial k(x_i,\\,x_j)}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{cov}\\left(\\dot{f}_i,\\, \\dot{f}_j\\right) = \\frac{\\partial k(x_i,\\,x_j)}{\\partial x_i \\partial x_j}\n",
    "$$\n",
    "\n",
    "where $\\dot{f}_i$ is the derivative of the process at $x_i$.\n",
    "Since `jax` can easily compose derivatives, it is straightforward to implement such a model with `tinygp`.\n",
    "\n",
    "In this case, our data points will each by a tuple with the `x` coordinate, and a boolean flag that is `True` when that observation is of the derivative of the process.\n",
    "An important thing to note here is that the {func}`tinygp.kernels.Kernel.evaluate` method always operates on a single pair of inputs.\n",
    "This means that in `evaluate`, you can unpack `(x, flag) = X` where `x` is the input coordinate and `flag` is a boolean flag.\n",
    "\n",
    "To begin, let's simulate some data [following this example](https://github.com/cornellius-gp/gpytorch/blob/e06201c7d40feb14080f1fdd1ffd41a6159c69fe/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_1d.ipynb) from the [GPyTorch](https://gpytorch.ai/) documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = np.linspace(0.0, 5 * np.pi, 50)\n",
    "y = np.concatenate(\n",
    "    (\n",
    "        np.sin(2 * X) + np.cos(X),\n",
    "        -np.sin(X) + 2 * np.cos(2 * X),\n",
    "    )\n",
    ")\n",
    "flag = np.concatenate((np.zeros(len(X), dtype=bool), np.ones(len(X), dtype=bool)))\n",
    "X = np.concatenate((X, X))\n",
    "y += 0.1 * np.random.default_rng(1234).normal(size=len(y))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "axes[0].plot(X[~flag], y[~flag], \".k\", label=\"value\")\n",
    "axes[1].plot(X[flag], y[flag], \".k\", label=\"derivative\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"value ($f$)\")\n",
    "_ = axes[1].set_ylabel(r\"derivative ($\\dot{f}$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we implement the kernel for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "class DerivativeKernel(tinygp.kernels.Kernel):\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def evaluate(self, X1, X2):\n",
    "        t1, d1 = X1\n",
    "        t2, d2 = X2\n",
    "\n",
    "        # Differentiate the kernel function: the first derivative wrt x1\n",
    "        Kp = jax.grad(self.kernel.evaluate, argnums=0)\n",
    "\n",
    "        # ... and the second derivative\n",
    "        Kpp = jax.grad(Kp, argnums=1)\n",
    "\n",
    "        # Evaluate the kernel matrix and all of its relevant derivatives\n",
    "        K = self.kernel.evaluate(t1, t2)\n",
    "        d2K_dx1dx2 = Kpp(t1, t2)\n",
    "\n",
    "        # For stationary kernels, these are related just by a minus sign, but we'll\n",
    "        # evaluate them both separately for generality's sake\n",
    "        dK_dx2 = jax.grad(self.kernel.evaluate, argnums=1)(t1, t2)\n",
    "        dK_dx1 = Kp(t1, t2)\n",
    "\n",
    "        return jnp.where(\n",
    "            d1, jnp.where(d2, d2K_dx1dx2, dK_dx1), jnp.where(d2, dK_dx2, K)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this definition, we can plot what the kernel functions look like for different base kernels, $k(x_i,\\,x_j)$ above.\n",
    "Don’t worry too much about the syntax here but, in these plots, we're showing all the covariances between $f$ and $\\dot{f}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel(base_kernel):\n",
    "    kernel = DerivativeKernel(base_kernel)\n",
    "\n",
    "    N = 500\n",
    "    dt = np.linspace(-7.5, 7.5, N)\n",
    "\n",
    "    k00 = kernel(\n",
    "        (jnp.zeros((1)), jnp.zeros((1), dtype=bool)),\n",
    "        (dt, np.zeros(N, dtype=bool)),\n",
    "    )[0]\n",
    "    k11 = kernel(\n",
    "        (jnp.zeros((1)), jnp.ones((1), dtype=bool)),\n",
    "        (dt, np.ones(N, dtype=bool)),\n",
    "    )[0]\n",
    "    k01 = kernel(\n",
    "        (jnp.zeros((1)), jnp.zeros((1), dtype=bool)),\n",
    "        (dt, np.ones(N, dtype=bool)),\n",
    "    )[0]\n",
    "    k10 = kernel(\n",
    "        (jnp.zeros((1)), jnp.ones((1), dtype=bool)),\n",
    "        (dt, np.zeros(N, dtype=bool)),\n",
    "    )[0]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(dt, k00, label=\"$\\mathrm{cov}(f,\\,f)$\", lw=1)\n",
    "    plt.plot(dt, k01, label=\"$\\mathrm{cov}(f,\\,\\dot{f})$\", lw=1)\n",
    "    plt.plot(dt, k10, label=\"$\\mathrm{cov}(\\dot{f},\\,f)$\", lw=1)\n",
    "    plt.plot(dt, k11, label=\"$\\mathrm{cov}(\\dot{f},\\,\\dot{f})$\", lw=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(r\"$\\Delta t$\")\n",
    "    plt.xlim(dt.min(), dt.max())\n",
    "\n",
    "\n",
    "plot_kernel(tinygp.kernels.Matern52(scale=1.5))\n",
    "plt.title(\"Matern-5/2\")\n",
    "\n",
    "plot_kernel(\n",
    "    tinygp.kernels.ExpSquared(scale=2.5)\n",
    "    * tinygp.kernels.ExpSineSquared(scale=2.5, gamma=0.5)\n",
    ")\n",
    "_ = plt.title(\"Quasiperiodic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit our simulated data using this kernel.\n",
    "Note, especially that we pass in `(X, flag)` as the input coordinates for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    base_kernel = jnp.exp(2 * params[\"log_amp\"]) * tinygp.kernels.ExpSquared(\n",
    "        scale=jnp.exp(params[\"log_scale\"])\n",
    "    )\n",
    "    kernel = DerivativeKernel(base_kernel)\n",
    "\n",
    "    # Note that we're passing in (X, flag) as the input coordinates.\n",
    "    return tinygp.GaussianProcess(kernel, (X, flag), diag=jnp.exp(params[\"log_diag\"]))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_scale\": np.log(1.5),\n",
    "    \"log_amp\": np.log(1.0),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0, 5 * np.pi, 500)\n",
    "gp = build_gp(soln.params)\n",
    "\n",
    "# Predict the function values for the function and its derivative\n",
    "mu1 = gp.condition(y, (X_grid, np.zeros(len(X_grid), dtype=bool))).gp.loc\n",
    "mu2 = gp.condition(y, (X_grid, np.ones(len(X_grid), dtype=bool))).gp.loc\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(X_grid, mu1)\n",
    "axes[0].plot(X[~flag], y[~flag], \".k\")\n",
    "axes[1].plot(X_grid, mu2)\n",
    "axes[1].plot(X[flag], y[flag], \".k\", label=\"derivative\")\n",
    "\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"value ($f$)\")\n",
    "_ = axes[1].set_ylabel(r\"derivative ($\\dot{f}$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(derivative-latent)=\n",
    "\n",
    "## A more detailed example: A latent GP & derivative\n",
    "\n",
    "Building on the previous example, we will next build a custom kernel that is commonly used when studying radial velocity observations of exoplanet-hosting stars, as described by [Rajpaul et al. (2015)](https://arxiv.org/abs/1506.07304).\n",
    "Take a look at that paper for more details about the math, but the tl;dr is that we want to model a set parallel, but qualitatively different, time series, using a latent Gaussian process.\n",
    "The interesting part of this model is that Rajpaul et al. model the observations as arbitrary linear combinations of the process and its first time derivative, and they work through the derivation of the resulting kernel function.\n",
    "\n",
    "In this tutorial, we will implement this kernel using `tinygp`—something that is significantly more annoying to do with other Gaussian process frameworks (trust me, I've tried!)—and demonstrate a few key features along the way.\n",
    "\n",
    "### The kernel\n",
    "\n",
    "The kernel matrix described by [Rajpaul et al. (2015)](https://arxiv.org/abs/1506.07304) is a block matrix where each element is a linear combination of the latent kernel and its first and second derivatives, where the relevant coefficients depend on the \"class\" of each pair of observations.\n",
    "This means that our input data needs to include, at each observation, the time `t` (our input coordinate in this case) and an integer class label `label`.\n",
    "As above, we will structure our data in such a way that we can treat each input as being a tuple `(t, label)`.\n",
    "In this case, we will unpack our inputs `t, label = X` and treat `t` and `label` as scalars.\n",
    "Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentKernel(tinygp.kernels.Kernel):\n",
    "    \"\"\"A custom kernel based on Rajpaul et al. (2015)\n",
    "\n",
    "    Args:\n",
    "        kernel: The kernel function describing the latent process. This can be any other\n",
    "            ``tinygp`` kernel.\n",
    "        coeff_prim: The primal coefficients for each class. This can be thought of as how\n",
    "            much the latent process itself projects into the observations for that class.\n",
    "            This should be an array with an entry for each class of observation.\n",
    "        coeff_deriv: The derivative coefficients for each class. This should have the same\n",
    "            shape as ``coeff_prim``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, coeff_prim, coeff_deriv):\n",
    "        self.kernel = kernel\n",
    "        self.coeff_prim, self.coeff_deriv = jnp.broadcast_arrays(\n",
    "            jnp.asarray(coeff_prim), jnp.asarray(coeff_deriv)\n",
    "        )\n",
    "\n",
    "    def evaluate(self, X1, X2):\n",
    "        t1, label1 = X1\n",
    "        t2, label2 = X2\n",
    "\n",
    "        # Differentiate the kernel function: the first derivative wrt x1\n",
    "        Kp = jax.grad(self.kernel.evaluate, argnums=0)\n",
    "\n",
    "        # ... and the second derivative\n",
    "        Kpp = jax.grad(Kp, argnums=1)\n",
    "\n",
    "        # Evaluate the kernel matrix and all of its relevant derivatives\n",
    "        K = self.kernel.evaluate(t1, t2)\n",
    "        d2K_dx1dx2 = Kpp(t1, t2)\n",
    "\n",
    "        # For stationary kernels, these are related just by a minus sign, but we'll\n",
    "        # evaluate them both separately for generality's sake\n",
    "        dK_dx2 = jax.grad(self.kernel.evaluate, argnums=1)(t1, t2)\n",
    "        dK_dx1 = Kp(t1, t2)\n",
    "\n",
    "        # Extract the coefficients\n",
    "        a1 = self.coeff_prim[label1]\n",
    "        a2 = self.coeff_prim[label2]\n",
    "        b1 = self.coeff_deriv[label1]\n",
    "        b2 = self.coeff_deriv[label2]\n",
    "\n",
    "        # Construct the matrix element\n",
    "        return a1 * a2 * K + a1 * b2 * dK_dx2 + b1 * a2 * dK_dx1 + b1 * b2 * d2K_dx1dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Given this custom kernel definition, let's simulate some data and fit for the kernel parameters.\n",
    "In this case, we'll use the product of an {class}`kernels.ExpSquared` and a {class}`kernels.ExpSineSquared` kernel for our latent process.\n",
    "Note that we're not including an amplitude parameter in our latent process, since that will be captured by the coefficients in our `LatentKernel` defined above.\n",
    "Using this latent process, we simulate an unbalanced dataset with two classes and plot the simulated data below, with class offsets for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kernel = tinygp.kernels.ExpSquared(scale=1.5) * tinygp.kernels.ExpSineSquared(\n",
    "    scale=2.5, gamma=0.5\n",
    ")\n",
    "kernel = LatentKernel(base_kernel, [1.0, 0.5], [-0.1, 0.3])\n",
    "\n",
    "random = np.random.default_rng(5678)\n",
    "t1 = np.sort(random.uniform(0, 10, 200))\n",
    "label1 = np.zeros_like(t1, dtype=int)\n",
    "t2 = np.sort(random.uniform(0, 10, 300))\n",
    "label2 = np.ones_like(t2, dtype=int)\n",
    "X = (np.append(t1, t2), np.append(label1, label2))\n",
    "\n",
    "gp = tinygp.GaussianProcess(kernel, X, diag=1e-5)\n",
    "y = gp.sample(jax.random.PRNGKey(1234))\n",
    "\n",
    "subset = np.append(\n",
    "    random.integers(len(t1), size=50),\n",
    "    len(t1) + random.integers(len(t2), size=15),\n",
    ")\n",
    "X_obs = (X[0][subset], X[1][subset])\n",
    "y_obs = y[subset] + 0.1 * random.normal(size=len(subset))\n",
    "\n",
    "offset = 2.5\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t1, y[: len(t1)] + 0.5 * offset, label=\"class 0\")\n",
    "plt.plot(t2, y[len(t1) :] - 0.5 * offset, label=\"class 1\")\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the simulated data, by optimizing for the maximum likelihood kernel parameters using `jaxopt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "\n",
    "def build_gp(params):\n",
    "    base_kernel = tinygp.kernels.ExpSquared(\n",
    "        scale=jnp.exp(params[\"log_scale\"])\n",
    "    ) * tinygp.kernels.ExpSineSquared(\n",
    "        scale=jnp.exp(params[\"log_period\"]), gamma=params[\"gamma\"]\n",
    "    )\n",
    "    kernel = LatentKernel(base_kernel, params[\"coeff_prim\"], params[\"coeff_deriv\"])\n",
    "    return tinygp.GaussianProcess(kernel, X_obs, diag=jnp.exp(params[\"log_diag\"]))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params):\n",
    "    gp = build_gp(params)\n",
    "    return -gp.log_probability(y_obs)\n",
    "\n",
    "\n",
    "init = {\n",
    "    \"log_scale\": np.log(1.5),\n",
    "    \"log_period\": np.log(2.5),\n",
    "    \"gamma\": np.float64(0.5),\n",
    "    \"coeff_prim\": np.array([1.0, 0.5]),\n",
    "    \"coeff_deriv\": np.array([-0.1, 0.3]),\n",
    "    \"log_diag\": np.log(0.1),\n",
    "}\n",
    "print(f\"Initial negative log likelihood: {loss(init)}\")\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(init)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the resulting inference.\n",
    "Of particular note here, even for the sparsely sampled \"class 1\" dataset, we get robust predictions for the expected process, since we have more finely sampled observations in \"class 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params)\n",
    "gp_cond = gp.condition(y_obs, X).gp\n",
    "mu, var = gp_cond.loc, gp_cond.variance\n",
    "\n",
    "plt.axhline(0.5 * offset, color=\"k\", lw=1)\n",
    "plt.axhline(-0.5 * offset, color=\"k\", lw=1)\n",
    "\n",
    "plt.plot(t1, y[: len(t1)] + 0.5 * offset, \"k\", label=\"truth\")\n",
    "plt.plot(t2, y[len(t1) :] - 0.5 * offset, \"k\")\n",
    "\n",
    "for c in [0, 1]:\n",
    "    delta = offset * (0.5 - c)\n",
    "    m = X[1] == c\n",
    "    plt.fill_between(\n",
    "        X[0][m],\n",
    "        delta + mu[m] + 2 * np.sqrt(var[m]),\n",
    "        delta + mu[m] - 2 * np.sqrt(var[m]),\n",
    "        color=f\"C{c}\",\n",
    "        alpha=0.5,\n",
    "        label=f\"inferred class {c}\",\n",
    "    )\n",
    "\n",
    "plt.plot(X_obs[0], y_obs + offset * (0.5 - X_obs[1]), \".k\", label=\"measured\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.1 * offset, 1.1 * offset)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y + offset\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d20ea8a315da34b3e8fab0dbd7b542a0ef3c8cf12937343660e6bc10a20768e3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('tinygp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
